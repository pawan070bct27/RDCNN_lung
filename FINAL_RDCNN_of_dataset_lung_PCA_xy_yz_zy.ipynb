{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "FINAL_RDCNN of dataset_lung_PCA_xy_yz_zy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawan070bct27/RDCNN_lung/blob/main/FINAL_RDCNN_of_dataset_lung_PCA_xy_yz_zy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwkASYB9Fx1G"
      },
      "source": [
        "# Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ayIPrEuFx1K"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Dropout, GlobalAveragePooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import AveragePooling2D\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, accuracy_score, precision_score, recall_score\n",
        "from collections import deque\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pp9E-9rFx1L"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB1qRDn3Fx1L"
      },
      "source": [
        "data_xy = np.load('train_xy.npy')\n",
        "label_xy = np.load('train_label_xy.npy')\n",
        "\n",
        "data_yz = np.load('train_yz.npy')\n",
        "label_yz = np.load('train_label_yz.npy')\n",
        "\n",
        "data_xz = np.load('train_xz.npy')\n",
        "label_xz = np.load('train_label_xz.npy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "982_VCi7txVw"
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(data_xy)\n",
        "data_xy_std = sc.transform(data_xy)\n",
        "sc.fit(data_xz)\n",
        "data_xz_std = sc.transform(data_xz)\n",
        "sc.fit(data_yz)\n",
        "data_yz_std = sc.transform(data_yz)\n",
        "#X_test_std = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdhf_XkQFx1M"
      },
      "source": [
        "#Making a list of dictionaries\n",
        "datasets = [\n",
        "{\n",
        "    'name': 'XY',\n",
        "    'data': data_xy_std,\n",
        "    'label': label_xy\n",
        "},\n",
        "{\n",
        "    'name': 'YZ',\n",
        "    'data': data_yz_std,\n",
        "    'label': label_yz\n",
        "},\n",
        "{\n",
        "    'name': 'XZ',\n",
        "    'data': data_xz_std,\n",
        "    'label': label_xz\n",
        "  \n",
        "}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "5iONhXy5SQL7",
        "outputId": "5424c925-3e7d-4310-85fa-42ca8c2b3d7e"
      },
      "source": [
        "#PCA visualization\n",
        "for i in datasets:\n",
        "  visualize_pca_obj = PCA()\n",
        "  visualize_pca = visualize_pca_obj.fit_transform(i['data'])\n",
        "  plt.bar(range(1, 16), visualize_pca_obj.explained_variance_ratio_[:15], alpha=0.5, align='center')\n",
        "  plt.step(range(1, 16), np.cumsum(visualize_pca_obj.explained_variance_ratio_)[:15], where='mid')\n",
        "  plt.ylabel('Explained variance ratio')\n",
        "  plt.xlabel('Principal component')\n",
        "  plt.title('For data : ' + str(i['name']))\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAc9UlEQVR4nO3debhcVZnv8e+PcCFMASFpOwMhsTuhO3BlMDKIXgKIl0GIT8sY5uYSHpBRkcbhAmJfG0UbsKVBZIgyCGFoSOgwydS2AmaABBIE05EhIUhEIAQQCLz3j71OdXFSp2rnpHbtU3V+n+ep5+xh7VXvKch5a+219lqKCMzMzADWKjsAMzPrO5wUzMyswknBzMwqnBTMzKzCScHMzCqcFMzMrMJJwawHkkZJCklrlx2LWas4KVjbkfSspLclrah6DSs5pgmSFhdY/36SXpK0adWxiZKWSNpY0te7fR4rJL2ZktqRRcVlncdJwdrVfhGxYdXrxdW5uN2+/UfEdOB+4EIASZsAlwInRMTrEfGdbp/HhqnsAuCW0gK3tuOkYB1D0rqSLpL0YnpdJGnddG6CpMWS/kHSS8DVNa4fIOn7kv4oaRGwb7fzx0h6StIbkhZJOj4d3wC4ExhW3XKRtIOkhyW9JmmppB9JWmcNfsVTgL0l/W+yP/gPRcS0Hj6LfVL5AyLizTV4T+tnnBSsk3wD2AnYFtgG2AH4ZtX5vwQ2BbYAJte4/jjg88B2wHjggG7nX07nBwHHABdK2j790d0beLFby+V94HRgMLAzsAdwYk/BS5onaVJP5yPij8CpwHUpjlN6qGcUcA1wXEQ81VN9ZrU4KVi7ui19A39N0m3p2GHAeRHxckQsA74FHFF1zQfAORHxTkS8XaPOg4CLIuKFiPgT8E/VJyPi3yPivyLzEHAP8JmeAoyI2RHxSESsjIhngR8Du9Yp//GIuL7B7/0IsDFwT/odPyS1jG4GrouIGxvUZbYKJwVrV1+IiE3S6wvp2DDguaoyz6VjXZZFxJ/r1DkMeKHb9RWS9pb0iKQ/SXoN2IesFVCTpLGS7kgdxMuB79Qrn9PlwM+AfSTtXOP8xcB7wFfW8H2sn3JSsE7yItmtoS4j07EujaYEXgps3u16oPIN/Bbg+8BHI2ITYAagOnVfCvwWGBMRg4CvV5VfbZKOTfGdmOq6orqPQtIRwBeBgyLivd6+j/VvTgrWSX4OfFPSEEmDgbOBa1fj+qnAKZJGSPoIcFbVuXWAdYFlwEpJewOfqzr/B2AzSRtXHdsIWA6skPQ3wAmr/RslacjtBWT9BO8AlwGvkPWjIGlr4F+BwyLihR4rMmvAScE6yT8Cs4B5wBPAnHQsr58AdwNz07W3dp2IiDfIOnanAq8Ck4BpVed/S5aUFqV+jmHAGancG6nuuvf4Jc2XdFgPp/8VuCEifpneL8g6xk+TtBXwZWAD4NYazyt8fTU+A+vn5EV2zMysi1sKZmZW4aRgZmYVTgpmZlbhpGBmZhVtNSkYwODBg2PUqFFlh2Fm1lZmz579x4gY0qhc2yWFUaNGMWvWrLLDMDNrK5Kea1zKt4/MzKyKk4KZmVU4KZiZWYWTgpmZVTgpmJlZhZOCmZlVOCmYmVmFk4KZmVW03cNrZmbt4vpHn+f2x5fkKjtu2CDO2W+rgiNqzEnBzCxZnT/ieTz6+z8BsOPoTZtWZ9GcFMzMktsfX8KCpcsZN3RQU+rbcfSmTNx2OJN2HNm4cB/hpGBmbanZ3+qBSkK48fidm1pvO3FHs5m1pa5v9c00buggJm47vKl1thu3FMyscP5W3z7cUjCzwvlbfftwS8HMVtHsb/b+Vt8+3FIws1U0+5u9v9W3D7cUzNqc79dbM7mlYNbmfL/emsktBbMO4G/11ixOCmYtVlQnrlkz+PaRWYu5E9f6MrcUzErg2z3WVzkpmNVR5Mges76o0NtHkvaS9LSkhZLOqnF+pKQHJD0maZ6kfYqMx2x1eWSP9TeFtRQkDQAuAfYEFgMzJU2LiAVVxb4JTI2ISyWNA2YAo4qKyaw3fKvH+pMiWwo7AAsjYlFEvAvcAEzsViaArnb0xsCLBcZjZmYNFNmnMBx4oWp/MbBjtzLnAvdIOhnYAPhsrYokTQYmA4wc2T6LVVjrebin2Zope0jqocCUiBgB7ANcI2mVmCLi8ogYHxHjhwwZ0vIgrX14uKfZmimypbAE2Lxqf0Q6Vu1YYC+AiHhY0kBgMPBygXFZh3MfgFnvFdlSmAmMkTRa0jrAIcC0bmWeB/YAkPS3wEBgWYExmZlZHYW1FCJipaSTgLuBAcBVETFf0nnArIiYBnwF+Imk08k6nY+OiCgqJutb/AyAWd9T6MNrETGDbJhp9bGzq7YXALsUGYP1XV33/5v5R9x9AGZrxk80W6l8/9+sbyl79JGZmfUhTgpmZlbhpGBmZhXuU7BcPFLIrH9wS8Fy8WyhZv2DWwqWm0cKmXU+txTMzKzCScHMzCqcFMzMrMJJwczMKpwUzMyswqOPOpRXIDOz3nBLoUN5BTIz6w23FDqYnysws9XlloKZmVU0TAqSRkj6N0nLJL0s6RZJI1oRnJmZtVaelsLVZGsrDwWGAdPTMTMz6zB5ksKQiLg6Ilam1xRgSMFxmZlZCfIkhVckHS5pQHodDrxSdGBmZtZ6eZLC3wMHAS8BS4EDgGOKDMrMzMrRcEhqRDwH7N+CWMzMrGQ9JgVJZ0bE9yT9CxDdz0fEKYVGZmZmLVevpfBU+jmrFYGYmVn5ekwKETE9bb4VETdVn5N0YKFRmZlZKfJMc/E14KYcx6yXmj15HXgCOzPrnXp9CnsD+wDDJf2w6tQgYGXRgfUnXZPXNfOPuCewM7PeqNdSeJGsP2F/YHbV8TeA04sMqj/y5HVm1hfU61OYC8yVdH1EvNfCmMzMrCR5+hRGSfonYBwwsOtgRHyssKjMzKwUeSfEu5SsH2E34GfAtUUGZWZm5ciTFNaLiPsARcRzEXEusG+xYZmZWRny3D56R9JawO8knQQsATYsNiwzMytDnpbCqcD6wCnAJ4DDgaOKDMrMzMpRt6UgaQBwcEScAazAs6OamXW0ui2FiHgf+HSLYjEzs5Ll6VN4TNI0smkt3uw6GBG3FhaVmZmVIk9SGEi20truVccCcFIwM+sweRbZ6XU/gqS9gIuBAcAVEXF+jTIHAeeSJZq5ETGpt+9nZmZrJk9LoVdSJ/UlwJ7AYmCmpGkRsaCqzBiyGVd3iYhXJf1FUfGYmVljeYak9tYOwMKIWBQR7wI3ABO7lTkOuCQiXgWIiJcLjMfMzBooMikMB16o2l+cjlUbC4yV9CtJj6TbTauQNFnSLEmzli1bVlC4ZmbWMClI+qikKyXdmfbHSTq2Se+/NjAGmAAcCvxE0ibdC0XE5RExPiLGDxkypElvbWZm3eVpKUwB7gaGpf1ngNNyXLcE2Lxqf0Q6Vm0xMC0i3ouI36e6x+So28zMCpAnKQyOiKnABwARsRJ4P8d1M4ExkkZLWgc4BJjWrcxtZK0EJA0mu520KF/oZmbWbHmSwpuSNiMbMoqknYDXG12UksdJZK2Mp4CpETFf0nmS9k/F7gZekbQAeAD4akS80ovfw8zMmiDPkNQvk33D/ytJvwKGAAfkqTwiZgAzuh07u2o7Uv1fzhuwmZkVJ8/Da3Mk7QpsCQh42stzmpl1pjyjj74EbBgR8yPiSWBDSScWH5qZmbVanj6F4yLita6d9KDZccWFZGZmZcmTFAZIUtdOmr5ineJCMjOzsuTpaL4LuFHSj9P+8emYmZl1mDxJ4R/IEsEJaf9e4IrCIjIzs9LkGX30AXBpepmZWQdrmBQk7UK23sEWqbzIHjH4WLGhmZlZq+W5fXQlcDowm3zTW5iZWZvKkxRej4g7C4+kjVz/6PPc/nj3uf16b8HS5YwbOqhp9ZmZ9VaeIakPSLpA0s6Stu96FR5ZH3b740tYsHR50+obN3QQE7ftvtSEmVnr5Wkp7Jh+jq86FsDuzQ+nfYwbOogbj9+57DDMzJoqz+ij3VoRiJmZlS9PSwFJ+wJbAQO7jkXEeUUFZWZm5cgzId5lwMHAyWTDUQ8kG55qZmYdJk9H86ci4kjg1Yj4FrAz2QppZmbWYfIkhbfTz7ckDQPeA4YWF5KZmZUlT5/CHZI2AS4A5pCNPPLcR2ZmHSjP6KNvp81bJN0BDIyIhms0m5lZ++kxKUjaPSLul/R3Nc4REbcWG5qZmbVavZbCrsD9wH41zgXgpGBm1mF6TAoRcY6ktYA7I2JqC2MyM7OS1B19lNZSOLNFsZiZWcnyDEn9haQzJG0uadOuV+GRmZlZy+UZknpw+vmlqmMBeJEdM7MOk2dI6uhWBGJmZuXLOyHe1sA4Pjwh3s+KCsrMzMqRZ43mc4AJZElhBrA38J+Ak4KZWYfJ09F8ALAH8FJEHANsA2xcaFRmZlaKXBPipaGpKyUNAl4GNi82LDMzK0OePoVZaUK8nwCzgRXAw4VGZWZmpcgz+ujEtHmZpLuAQRExr9iwzMysDHlWXpsmaZKkDSLiWScEM7POladP4QfAp4EFkm6WdICkgY0uMjOz9pPn9tFDwEOSBgC7A8cBVwGDCo7NzMxaLO/Da+uRTaF9MLA98NMigzIzs3LkeXhtKrADcBfwI+ChNETVzMw6TJ6WwpXAoRHxftHBmJlZuRp2NEfE3b1NCJL2kvS0pIWSzqpT7ouSQtL43ryPmZk1R57RR72SOqYvIZsraRxwqKRxNcptBJwKPFpULGZmlk9hSYGsH2JhRCyKiHeBG4CJNcp9G/gu8OcCYzEzsxx67FOQtH29CyNiToO6hwMvVO0vBnas8R6bR8S/S/pqnVgmA5MBRo4c2eBtzcyst+p1NP8g/RwIjAfmAgI+DswCdl6TN5a0FvDPwNGNykbE5cDlAOPHj481eV8zM+tZj7ePImK3iNgNWApsHxHjI+ITwHbAkhx1L+HDs6mO6HbdRsDWwIOSngV2Aqa5s9nMrDx5hqRuGRFPdO1ExJOS/jbHdTOBMZJGkyWDQ4BJVfW8Dgzu2pf0IHBGRMzKGXvTfWv6fBa8uLxhuQVLlzNuqB/oNrPOkycpzJN0BXBt2j8MaDgpXkSslHQScDcwALgqIuZLOg+YFRHTeht0b1147zN1zz/2/Gsse+OdumVGfGQ9xg0dxMRthzczNDOzPiFPUjgGOIFs2CjAfwCX5qk8ImaQLeFZfezsHspOyFNnkXYdO6RhmdP3HNuCSMzMypFnQrw/S7oMmBERT7cgJjMzK0me9RT2Bx4nm/sISdtKavmtHzMzK16eh9fOIXsQ7TWAiHgcGF1kUGZmVo48SeG9NFKomp8VMDPrQHk6mudLmgQMkDQGOAX4dbFhmZlZGfK0FE4GtgLeAX4OLAdOKzIoMzMrR57RR28B30gvMzPrYHlWXhsLnAGMqi4fEbsXF5aZmZUhT5/CTcBlwBWAV18zM+tgeZLCyojI9QSzmZm1tzwdzdMlnShpqKRNu16FR2ZmZi2Xp6VwVPpZvQhOAB9rfjhmZlamPKOP/PSymVk/UW85zt0j4n5Jf1frfETcWlxYZmZWhnothV2B+4H9apwLwEnBzKzD9JgUIuKc9POY1oVjZmZlytPRjKR9yaa6GNh1LCLOKyooMzMrR571FC4DDiabA0nAgcAWBcdlZmYlyPOcwqci4kjg1Yj4FrAz4DUpzcw6UJ6k8Hb6+ZakYcB7wNDiQjIzs7Lk6VO4Q9ImwAXAHLKRR1cUGpWZmZUiz8Nr306bt0i6AxhYYyU2MzPrAPUeXqv50Fo654fXzMw6UL2WQq2H1rr44TUzsw5U7+E1P7RmZtbP5HlOYTNJP5Q0R9JsSRdL2qwVwZmZWWvlGZJ6A7AM+CJwQNq+scigzMysHHmGpA6tGoEE8I+SDi4qIDMzK0+elsI9kg6RtFZ6HQTcXXRgZmbWenmSwnHA9cA76XUDcLykNyQtLzI4MzNrrTwPr23UikDMzKx8eUYfHdttf4Ckc4oLyczMypLn9tEekmZIGippa+ARwK0HM7MOlOf20aQ02ugJ4E1gUkT8qvDIzMys5fLcPhoDnArcAjwHHCFp/aIDMzOz1stz+2g68H8j4nhgV+B3wMxCozIzs1LkeXhth4hYDhARAfxA0vRiwzIzszL02FKQdCZARCyXdGC300cXGZSZmZWj3u2jQ6q2v9bt3F55Kpe0l6SnJS2UdFaN81+WtEDSPEn3SdoiT71mZlaMeklBPWzX2l/1YmkAcAmwNzAOOFTSuG7FHgPGR8THgZuB7zWM2MzMClMvKUQP27X2a9kBWBgRiyLiXbLpMSZ+qJKIByLirbT7CDAiR71mZlaQeh3N26S5jQSsVzXPkYCBOeoeDrxQtb8Y2LFO+WOBO3PUa2ZmBam38tqAVgUh6XBgPNmQ11rnJwOTAUaOHNmqsMzM+p08zyn01hJg86r9EenYh0j6LPANYP+IeKdWRRFxeUSMj4jxQ4YMKSRYMzMrNinMBMZIGi1pHbLRTNOqC0jaDvgxWUJ4ucBYzMwsh8KSQkSsBE4iW5DnKWBqRMyXdJ6k/VOxC4ANgZskPS5pWg/VmZlZC+R5ornXImIGMKPbsbOrtj9b5PubmdnqKfL2kZmZtRknBTMzq3BSMDOzCicFMzOrcFIwM7MKJwUzM6sodEhqf3Dhvc+s0fWn7zm2SZGYma05txTMzKzCScHMzCqcFMzMrMJJwczMKtzR3Mesacc1uPPazHrPLQUzM6twUjAzswonBTMzq3BSMDOzCicFMzOrcFIwM7MKJwUzM6twUjAzswonBTMzq3BSMDOzCicFMzOrcFIwM7MKJwUzM6twUjAzswonBTMzq3BSMDOzCicFMzOr8Mpr/cCarubmldzM+g+3FMzMrMJJwczMKpwUzMyswknBzMwq3NFsveLOa7PO5JaCmZlVuKVgfcKatjzArQ+zZnBLwczMKtxSsI7V7H4Pt2asPyg0KUjaC7gYGABcERHndzu/LvAz4BPAK8DBEfFskTGZ9SXtkLg8qKB/KSwpSBoAXALsCSwGZkqaFhELqoodC7waEX8t6RDgu8DBRcVkZn1Df0yG7dLSLLJPYQdgYUQsioh3gRuAid3KTAR+mrZvBvaQpAJjMjOzOhQRxVQsHQDsFRH/J+0fAewYESdVlXkylVmc9v8rlfljt7omA5PT7pbA02l7MPChsn2QY2wOx9g87RCnY2yO6hi3iIghjS5oi47miLgcuLz7cUmzImJ8CSHl5hibwzE2TzvE6RibozcxFnn7aAmwedX+iHSsZhlJawMbk3U4m5lZCYpMCjOBMZJGS1oHOASY1q3MNOCotH0AcH8UdT/LzMwaKuz2UUSslHQScDfZkNSrImK+pPOAWRExDbgSuEbSQuBPZIljdaxyS6kPcozN4Ribpx3idIzNsdoxFtbRbGZm7cfTXJiZWYWTgpmZVbRlUpC0l6SnJS2UdFbZ8dQiaXNJD0haIGm+pFPLjqknkgZIekzSHWXHUoukTSTdLOm3kp6StHPZMXUn6fT03/lJST+XNLAPxHSVpJfT80BdxzaVdK+k36WfH+mDMV6Q/lvPk/RvkjYpM8YU0ypxVp37iqSQNLiM2KriqBmjpJPT5zlf0vca1dN2SaFq+oy9gXHAoZLGlRtVTSuBr0TEOGAn4Et9NE6AU4Gnyg6ijouBuyLib4Bt6GOxShoOnAKMj4ityQZWrO6giSJMAfbqduws4L6IGAPcl/bLNIVVY7wX2DoiPg48A3yt1UHVMIVV40TS5sDngOdbHVANU+gWo6TdyGaO2CYitgK+36iStksK5Js+o3QRsTQi5qTtN8j+kA0vN6pVSRoB7AtcUXYstUjaGPhfZCPViIh3I+K1cqOqaW1gvfS8zfrAiyXHQ0T8B9movmrVU8v8FPhCS4PqplaMEXFPRKxMu4+QPeNUqh4+S4ALgTOB0kfs9BDjCcD5EfFOKvNyo3raMSkMB16o2l9MH/xjW03SKGA74NFyI6npIrL/qT8oO5AejAaWAVenW1xXSNqg7KCqRcQSsm9gzwNLgdcj4p5yo+rRRyNiadp+CfhomcHk8PfAnWUHUYukicCSiJhbdix1jAU+I+lRSQ9J+mSjC9oxKbQVSRsCtwCnRcTysuOpJunzwMsRMbvsWOpYG9geuDQitgPepPxbHh+S7stPJEtgw4ANJB1eblSNpQdFS/+G2xNJ3yC7DXtd2bF0J2l94OvA2WXH0sDawKZkt7C/CkxtNOloOyaFPNNn9AmS/gdZQrguIm4tO54adgH2l/Qs2W243SVdW25Iq1gMLI6IrlbWzWRJoi/5LPD7iFgWEe8BtwKfKjmmnvxB0lCA9LPh7YQySDoa+DxwWB+d5eCvyL4EzE3/fkYAcyT9ZalRrWoxcGtkfkN2R6Buh3g7JoU802eULmXjK4GnIuKfy46nloj4WkSMiIhRZJ/j/RHRp77hRsRLwAuStkyH9gAW1LmkDM8DO0laP/1334M+1hlepXpqmaOA20uMpaa0ONeZwP4R8VbZ8dQSEU9ExF9ExKj072cxsH36/7UvuQ3YDUDSWGAdGszs2nZJIXVAdU2f8RQwNSLmlxtVTbsAR5B9+348vfYpO6g2dTJwnaR5wLbAd0qO50NSK+ZmYA7wBNm/q9KnQJD0c+BhYEtJiyUdC5wP7Cnpd2QtnPPr1VFSjD8CNgLuTf9uLiszRugxzj6lhxivAj6WhqneABzVqOXlaS7MzKyi7VoKZmZWHCcFMzOrcFIwM7MKJwUzM6twUjAzswonBSudpPfT0MMnJd2UnhatVe7Xvax/vKQfrkF8K3p7bTuRdFpPn731Hx6SaqWTtCIiNkzb1wGzqx/4k7R21QRppcbXydKTueMjou7DTdbZ3FKwvuaXwF9LmiDpl5KmkZ5g7vrGns49WLXGwnVd87lI+qSkX0uaK+k3kjZK5e9I58+VdI2kh9OaAsel4xtKuk/SHElPpMnO6pJ0ZJrzf66ka9KxUZLuT8fvkzQyHZ8i6VJJj0halGK6Stn6EFOq6lwh6UJlc9/fJ2lIOr5turZrjYGPpOMPSvpu+l2fkfSZdHyAsnUJZqZrjq/32Uk6hWzepgckPdCE/47WriLCL79KfQEr0s+1yaZdOAGYQDb53ega5SYAr5PNN7MW2VOcnyZ7hH8R8MlUblCqcwJwRzp2LjAXWI9sDpgXyP4Yrg0MSmUGAwv575b0ihoxb0U21//gtL9p+jmd7KlRyGb4vC1tTyF7olRkk+ctB/5nin82sG0qF2Tz/UA22dqP0vY8YNe0fR5wUdp+EPhB2t4H+EXangx8M22vC8wim6un5meXyj3b9fv41X9fbilYX7CepMfJ/nA9T1o7AfhNRPy+h2t+ExGLI+ID4HFgFLAlsDQiZgJExPKofdvp9oh4O7LbJA+QrdEh4DtpKo1fkE3HXm9a6d2Bm1IdRETXPPY7A9en7WvIklWX6RERZFNh/CGy+XM+AOan+CGbsOzGtH0t8Glla0psEhEPpeM/JVtjokvXZIuzq+r5HHBk+lwfBTYDxqRztT47MyD7dmRWtrcjYtvqA+lu0Jt1rnmnavt9Vu//5e4daQEcBgwBPhER76X7681eUrMr5g/4cPwf0HP8eTr9uuqq/hwEnBwRd1cXlDSBNfvsrMO5pWCd5GlgqNJCIqk/odYfvImSBkrajOx2ykxgY7K1Jd5TtoThFg3e637gwFQHkjZNx3/Nfy/FeRhZH8nqWAs4IG1PAv4zIl4HXu3qLyCbaPGhWhdXuRs4Qdn07Ugaq8aLE71BNhGd9WP+hmAdIyLelXQw8C+S1gPeJpsJtLt5ZLeNBgPfjogX06in6ZKeILuN9dsG7zVf0v8DHpL0PvAYcDTZjK5XS/oq2Ypxx6zmr/EmsIOkb5KtdXBwOn4UcFkaMrooR71XkN0WmpM64ZfReOnNy4G7JL0YEbutZtzWITwk1foVSeeSdRw3XMC8DP1l+Kv1Xb59ZGZmFW4pmJlZhVsKZmZW4aRgZmYVTgpmZlbhpGBmZhVOCmZmVvH/AYAitGm3c8mcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTzAkYlXFx1N"
      },
      "source": [
        "#defining a function for splitting datasets after application of PCA on them\n",
        "def split_datasets():\n",
        "  pca = PCA()\n",
        "  pca_obj = pca.fit_transform(m['data'])\n",
        "  pca_obj = pca_obj[:,:6]\n",
        "  pca_obj = pca_obj.reshape(167,6,6)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(pca_obj, m['label'].reshape(167,6,1), test_size=0.3, random_state=101, shuffle = True)\n",
        "\n",
        "  X_train = X_train.reshape(X_train.shape[0]*6, 6)\n",
        "  y_train = y_train.reshape(y_train.shape[0]*6,)\n",
        "\n",
        "  X_test = X_test.reshape(X_test.shape[0]*6, 6)\n",
        "  y_test = y_test.reshape(y_test.shape[0]*6,)\n",
        "\n",
        "    \n",
        "  X_train = X_train.reshape(696, 6, 1,1)\n",
        "  X_test = X_test.reshape(306, 6, 1,1)\n",
        "\n",
        "  y_train = np_utils.to_categorical(y_train, 2)\n",
        "  y_test = np_utils.to_categorical(y_test, 2) \n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pupae2LvTOF"
      },
      "source": [
        "#Designing a model\n",
        "from keras.layers import *\n",
        "\n",
        "def makemodel():\n",
        "  model = Build_model()\n",
        "  return model\n",
        "\n",
        "def Build_model():\n",
        "  X_raw, y_raw = X_train, y_train\n",
        "  n_class = 2\n",
        "  from keras.models import Model\n",
        "  from keras import optimizers\n",
        "\n",
        "  def RCL_block(filedepth, input):\n",
        "      conv1 = Conv2D(filters=filedepth, kernel_size=[3, 3], strides=(1, 1), padding='same',activation='relu')(input)\n",
        "      stack2 = BatchNormalization()(conv1)\n",
        "\n",
        "      RCL = Conv2D(filters=filedepth, kernel_size=[3, 3], strides=(1, 1), padding='same', activation='relu')\n",
        "\n",
        "      conv2 = RCL(stack2)\n",
        "      stack3 = Add()([conv1, conv2])\n",
        "      stack4 = BatchNormalization()(stack3)\n",
        "\n",
        "      conv3 = Conv2D(filters=filedepth, kernel_size=[3, 3], strides=(1, 1), padding='same',activation='relu', weights=RCL.get_weights())(stack4)\n",
        "      stack5 =  Add()([conv1, conv3])\n",
        "      stack6 = BatchNormalization()(stack5)\n",
        "\n",
        "      conv4 = Conv2D(filters=filedepth, kernel_size=[3, 3], strides=(1, 1), padding='same',activation='relu', weights=RCL.get_weights())(stack6)\n",
        "      stack7 =  Add()([conv1, conv4])\n",
        "      stack8 = BatchNormalization()(stack7)\n",
        "\n",
        "      return stack8\n",
        "\n",
        "  input_img = Input(shape=(6, 1, 1))\n",
        "  conv1 = Conv2D(filters=32, kernel_size=[3,3], strides=(1, 1), padding='same',activation='relu')(input_img)\n",
        "\n",
        "  rconv1 = RCL_block(64, conv1)\n",
        "  dropout1 = Dropout(0.2)(rconv1)\n",
        "  rconv2 = RCL_block(64, dropout1)\n",
        "  maxpooling_1 = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(rconv2)\n",
        "  dropout2 = Dropout(0.2)(maxpooling_1)\n",
        "  rconv3 = RCL_block(64, dropout2)\n",
        "  dropout3 = Dropout(0.2)(rconv3)\n",
        "  rconv4 = RCL_block(64, dropout3)\n",
        "\n",
        "  out = MaxPool2D((2, 2), strides=(2, 2), padding='same')(rconv4)\n",
        "  flatten = Flatten()(out)\n",
        "  prediction = Dense(2, activation='softmax')(flatten)\n",
        "\n",
        "  model = Model(inputs=input_img, outputs=prediction)\n",
        "  adam = optimizers.Adam()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLzf_81DOQWe",
        "outputId": "9032bd78-5718-4108-95f8-d033d96a9753"
      },
      "source": [
        "import tensorflow as tf\n",
        "# Below for loop iterates through your dataset list\n",
        "#window = deque([],maxlen=2)\n",
        "speci = deque([])\n",
        "sensi = deque([])\n",
        "au    = deque([])\n",
        "_f1score = deque([])\n",
        "_accuracy = deque([])\n",
        "\n",
        "# NAME = \"Lung_cancer_typical_CNN{}\".format(int(time.time()))\n",
        "# tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
        "\n",
        "\n",
        "for m in datasets:\n",
        "# Define per-fold score containers\n",
        "    acc_per_fold = []\n",
        "    loss_per_fold = []\n",
        "    X_train, X_test, y_train, y_test = split_datasets()\n",
        "  \n",
        "    # Merge inputs and targets\n",
        "    inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "    targets = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "    # Define the K-fold Cross Validator\n",
        "    from sklearn.model_selection import KFold\n",
        "    kfold = KFold(n_splits= 5, shuffle=True)\n",
        "\n",
        "    # K-fold Cross Validation model evaluation\n",
        "    fold_no = 1\n",
        "    for train, test in kfold.split(inputs, targets):\n",
        "      \n",
        "      # Generate a print\n",
        "      print('------------------------------------------------------------------------')\n",
        "      print(f'Training for fold {fold_no} ...')\n",
        "      model = makemodel()\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      model.summary()\n",
        "      # Fit data to model\n",
        "      history1 = model.fit(inputs[train], targets[train], validation_data=(inputs[test], targets[test]), batch_size=4, epochs=50, verbose=1, shuffle=True)\n",
        "         \n",
        "      tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "      # Generate generalization metrics\n",
        "      scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "      print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "      acc_per_fold.append(scores[1] * 100)\n",
        "      loss_per_fold.append(scores[0])\n",
        "      # Increase fold numberÃ¥\n",
        "      fold_no = fold_no + 1\n",
        "    # == Provide average scores ==\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print('Score per fold')\n",
        "    for i in range(0, len(acc_per_fold)):\n",
        "      print('------------------------------------------------------------------------')\n",
        "      print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print('Average scores for all folds:')\n",
        "    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "    print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "    print('------------------------------------------------------------------------')\n",
        "    \n",
        "    converted_ytest = y_test.argmax(axis=1)\n",
        "    y_pred=model.predict(X_test)\n",
        "    # from sklearn.metrics import confusion_matrix\n",
        "    # cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "    # window.append(cm)\n",
        "# Compute False postive rate, and True positive rate\n",
        "    fpr, tpr, thresholds = roc_curve(converted_ytest, y_pred.argmax(axis=1))\n",
        " #Compute precision   \n",
        "    raw_speci= precision_score(converted_ytest, y_pred.argmax(axis=1))\n",
        "    speci.append(((raw_speci)))\n",
        "#Compute recall\n",
        "    raw_sensi= precision_score(converted_ytest, y_pred.argmax(axis=1))\n",
        "    sensi.append(((raw_sensi)))\n",
        "    # sensi.append(\"{:.0%}\".format(tpr))\n",
        "# Calculate Area under the curve to display on the plot\n",
        "    auc = roc_auc_score(converted_ytest, y_pred.argmax(axis = 1))\n",
        "    au.append(((auc)))\n",
        "#accuracy\n",
        "    _acc = accuracy_score(converted_ytest, y_pred.argmax(axis=1))\n",
        "    _accuracy.append((_acc))\n",
        "# F1 -score\n",
        "    _fscore1 = f1_score(converted_ytest, y_pred.argmax(axis=1))\n",
        "    _f1score.append(((_fscore1)))\n",
        "\n",
        "# Now, plot the computed values\n",
        "    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['name'], auc))\n",
        "# Custom settings for the plot \n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Specificity(False Positive Rate)')\n",
        "plt.ylabel('Sensitivity(True Positive Rate)')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show() \n",
        "  # Display\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 6, 1, 32)     320         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 6, 1, 64)     18496       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 6, 1, 64)     256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 6, 1, 64)     36928       batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 6, 1, 64)     0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 6, 1, 64)     256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 1, 64)     36928       batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 6, 1, 64)     0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 6, 1, 64)     256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 6, 1, 64)     36928       batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 6, 1, 64)     0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 6, 1, 64)     256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 6, 1, 64)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 6, 1, 64)     36928       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 6, 1, 64)     256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 6, 1, 64)     36928       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 6, 1, 64)     0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 6, 1, 64)     256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 6, 1, 64)     36928       batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 6, 1, 64)     0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 6, 1, 64)     256         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 6, 1, 64)     36928       batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 6, 1, 64)     0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 6, 1, 64)     256         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 3, 1, 64)     0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 3, 1, 64)     0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 3, 1, 64)     36928       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 3, 1, 64)     256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 3, 1, 64)     0           conv2d_9[0][0]                   \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 3, 1, 64)     256         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 3, 1, 64)     0           conv2d_9[0][0]                   \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 3, 1, 64)     256         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 3, 1, 64)     0           conv2d_9[0][0]                   \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 3, 1, 64)     256         add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 3, 1, 64)     0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 3, 1, 64)     36928       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 3, 1, 64)     256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 3, 1, 64)     0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 3, 1, 64)     256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 3, 1, 64)     0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 3, 1, 64)     256         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 3, 1, 64)     0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 3, 1, 64)     256         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 2, 1, 64)     0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 128)          0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            258         flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 5s 14ms/step - loss: 0.9526 - accuracy: 0.5595 - val_loss: 0.6250 - val_accuracy: 0.6866\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6623 - accuracy: 0.6509 - val_loss: 0.7675 - val_accuracy: 0.7363\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6413 - accuracy: 0.6612 - val_loss: 0.5582 - val_accuracy: 0.7463\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6091 - accuracy: 0.6579 - val_loss: 0.4786 - val_accuracy: 0.7811\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5761 - accuracy: 0.6994 - val_loss: 0.4399 - val_accuracy: 0.8109\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.4658 - accuracy: 0.7879 - val_loss: 0.4735 - val_accuracy: 0.8060\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.4491 - accuracy: 0.8080 - val_loss: 0.4719 - val_accuracy: 0.7960\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.4488 - accuracy: 0.8040 - val_loss: 0.4205 - val_accuracy: 0.8060\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4710 - accuracy: 0.7903 - val_loss: 0.4940 - val_accuracy: 0.8010\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.4556 - accuracy: 0.8084 - val_loss: 0.3701 - val_accuracy: 0.8408\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.4410 - accuracy: 0.8126 - val_loss: 0.4040 - val_accuracy: 0.8259\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3612 - accuracy: 0.8570 - val_loss: 0.5311 - val_accuracy: 0.8010\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3857 - accuracy: 0.8564 - val_loss: 0.3835 - val_accuracy: 0.8557\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6576 - accuracy: 0.6567 - val_loss: 0.4848 - val_accuracy: 0.7761\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4564 - accuracy: 0.8096 - val_loss: 0.5129 - val_accuracy: 0.7910\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.4081 - accuracy: 0.8209 - val_loss: 0.4015 - val_accuracy: 0.8458\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.4128 - accuracy: 0.8322 - val_loss: 0.4117 - val_accuracy: 0.8159\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.3982 - accuracy: 0.8201 - val_loss: 0.3929 - val_accuracy: 0.8209\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3662 - accuracy: 0.8474 - val_loss: 0.4427 - val_accuracy: 0.8060\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.3859 - accuracy: 0.8294 - val_loss: 0.3737 - val_accuracy: 0.8308\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.3525 - accuracy: 0.8544 - val_loss: 0.4140 - val_accuracy: 0.8109\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.3675 - accuracy: 0.8462 - val_loss: 0.3970 - val_accuracy: 0.8507\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3464 - accuracy: 0.8573 - val_loss: 0.5132 - val_accuracy: 0.8209\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3098 - accuracy: 0.8433 - val_loss: 0.3155 - val_accuracy: 0.8706\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2946 - accuracy: 0.8574 - val_loss: 0.3733 - val_accuracy: 0.8607\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.2317 - accuracy: 0.9144 - val_loss: 0.4001 - val_accuracy: 0.8159\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 2s 10ms/step - loss: 0.3099 - accuracy: 0.8762 - val_loss: 0.3424 - val_accuracy: 0.8458\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2734 - accuracy: 0.8853 - val_loss: 0.3570 - val_accuracy: 0.8607\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1862 - accuracy: 0.9362 - val_loss: 0.4456 - val_accuracy: 0.8060\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2785 - accuracy: 0.8752 - val_loss: 0.3481 - val_accuracy: 0.8209\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2843 - accuracy: 0.8916 - val_loss: 0.3908 - val_accuracy: 0.8557\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3010 - accuracy: 0.8835 - val_loss: 0.3519 - val_accuracy: 0.8159\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2362 - accuracy: 0.9049 - val_loss: 0.4357 - val_accuracy: 0.8358\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2844 - accuracy: 0.8711 - val_loss: 0.3048 - val_accuracy: 0.8806\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2551 - accuracy: 0.9042 - val_loss: 0.3878 - val_accuracy: 0.8408\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2633 - accuracy: 0.8920 - val_loss: 0.3545 - val_accuracy: 0.8607\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2238 - accuracy: 0.9145 - val_loss: 0.3316 - val_accuracy: 0.8507\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1950 - accuracy: 0.9210 - val_loss: 0.3253 - val_accuracy: 0.8507\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2064 - accuracy: 0.9110 - val_loss: 0.3042 - val_accuracy: 0.8706\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2660 - accuracy: 0.8887 - val_loss: 0.2815 - val_accuracy: 0.8806\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2274 - accuracy: 0.9121 - val_loss: 0.2806 - val_accuracy: 0.8706\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1988 - accuracy: 0.9199 - val_loss: 0.2678 - val_accuracy: 0.8955\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2268 - accuracy: 0.9281 - val_loss: 0.3589 - val_accuracy: 0.8557\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2256 - accuracy: 0.9112 - val_loss: 0.3312 - val_accuracy: 0.8607\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2070 - accuracy: 0.9196 - val_loss: 0.2731 - val_accuracy: 0.8955\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1430 - accuracy: 0.9571 - val_loss: 0.3746 - val_accuracy: 0.8408\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2071 - accuracy: 0.9164 - val_loss: 0.3199 - val_accuracy: 0.8856\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1596 - accuracy: 0.9407 - val_loss: 0.3008 - val_accuracy: 0.8756\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3398 - accuracy: 0.8627 - val_loss: 0.3157 - val_accuracy: 0.8856\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2111 - accuracy: 0.9168 - val_loss: 0.3204 - val_accuracy: 0.8557\n",
            "Score for fold 1: loss of 0.3204331398010254; accuracy of 85.57214140892029%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 6, 1, 32)     320         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 6, 1, 64)     18496       conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 6, 1, 64)     256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 6, 1, 64)     0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 6, 1, 64)     256         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 6, 1, 64)     0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 6, 1, 64)     256         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 6, 1, 64)     0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 6, 1, 64)     256         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 6, 1, 64)     0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 6, 1, 64)     36928       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 6, 1, 64)     256         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 6, 1, 64)     0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 6, 1, 64)     256         add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 6, 1, 64)     0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 6, 1, 64)     256         add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 6, 1, 64)     0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 6, 1, 64)     256         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 3, 1, 64)     0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 3, 1, 64)     0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 3, 1, 64)     36928       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 3, 1, 64)     256         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 3, 1, 64)     0           conv2d_26[0][0]                  \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 3, 1, 64)     256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 3, 1, 64)     0           conv2d_26[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 3, 1, 64)     256         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 3, 1, 64)     0           conv2d_26[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 3, 1, 64)     256         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 3, 1, 64)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 3, 1, 64)     36928       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 3, 1, 64)     256         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 3, 1, 64)     0           conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 3, 1, 64)     256         add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 3, 1, 64)     0           conv2d_30[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 3, 1, 64)     256         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 3, 1, 64)     0           conv2d_30[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 3, 1, 64)     256         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 2, 1, 64)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 128)          0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            258         flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 4s 12ms/step - loss: 0.9171 - accuracy: 0.5709 - val_loss: 0.6432 - val_accuracy: 0.7363\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6601 - accuracy: 0.6598 - val_loss: 0.6713 - val_accuracy: 0.6766\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6271 - accuracy: 0.6811 - val_loss: 0.6272 - val_accuracy: 0.6766\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6111 - accuracy: 0.7000 - val_loss: 0.5505 - val_accuracy: 0.7264\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5242 - accuracy: 0.7225 - val_loss: 0.4673 - val_accuracy: 0.7811\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5162 - accuracy: 0.7525 - val_loss: 0.5519 - val_accuracy: 0.7114\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5039 - accuracy: 0.7529 - val_loss: 0.4873 - val_accuracy: 0.7662\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4375 - accuracy: 0.8024 - val_loss: 0.4698 - val_accuracy: 0.7861\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3884 - accuracy: 0.8342 - val_loss: 0.5398 - val_accuracy: 0.7960\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3942 - accuracy: 0.8163 - val_loss: 0.6709 - val_accuracy: 0.6915\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4191 - accuracy: 0.8328 - val_loss: 0.4624 - val_accuracy: 0.7960\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3960 - accuracy: 0.8346 - val_loss: 0.4829 - val_accuracy: 0.8010\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3921 - accuracy: 0.8351 - val_loss: 0.3868 - val_accuracy: 0.8060\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.3632 - accuracy: 0.8421 - val_loss: 0.4171 - val_accuracy: 0.8308\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3617 - accuracy: 0.8489 - val_loss: 0.4333 - val_accuracy: 0.8358\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4807 - accuracy: 0.7794 - val_loss: 0.3467 - val_accuracy: 0.8657\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3990 - accuracy: 0.8301 - val_loss: 0.5799 - val_accuracy: 0.7612\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3659 - accuracy: 0.8456 - val_loss: 0.3416 - val_accuracy: 0.8557\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2895 - accuracy: 0.8882 - val_loss: 0.3520 - val_accuracy: 0.8308\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3059 - accuracy: 0.8833 - val_loss: 0.3362 - val_accuracy: 0.8657\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3348 - accuracy: 0.8666 - val_loss: 0.4441 - val_accuracy: 0.8060\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3024 - accuracy: 0.8730 - val_loss: 0.2776 - val_accuracy: 0.8557\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2706 - accuracy: 0.8972 - val_loss: 0.3692 - val_accuracy: 0.8209\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2629 - accuracy: 0.9014 - val_loss: 0.2898 - val_accuracy: 0.8905\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2855 - accuracy: 0.8824 - val_loss: 0.3375 - val_accuracy: 0.8657\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2524 - accuracy: 0.9027 - val_loss: 0.3283 - val_accuracy: 0.8458\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3020 - accuracy: 0.8654 - val_loss: 0.3521 - val_accuracy: 0.8358\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2825 - accuracy: 0.8810 - val_loss: 0.3385 - val_accuracy: 0.8358\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2260 - accuracy: 0.9105 - val_loss: 0.2489 - val_accuracy: 0.9104\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2297 - accuracy: 0.9214 - val_loss: 0.3326 - val_accuracy: 0.8657\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2389 - accuracy: 0.9152 - val_loss: 0.2820 - val_accuracy: 0.8806\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2135 - accuracy: 0.9301 - val_loss: 0.2528 - val_accuracy: 0.8955\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2322 - accuracy: 0.9213 - val_loss: 0.2362 - val_accuracy: 0.9154\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2260 - accuracy: 0.9239 - val_loss: 0.2560 - val_accuracy: 0.9005\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2421 - accuracy: 0.9065 - val_loss: 0.3345 - val_accuracy: 0.8557\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2473 - accuracy: 0.8952 - val_loss: 0.3017 - val_accuracy: 0.8756\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2346 - accuracy: 0.9122 - val_loss: 0.3685 - val_accuracy: 0.8756\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2375 - accuracy: 0.8968 - val_loss: 0.3003 - val_accuracy: 0.8706\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2026 - accuracy: 0.9213 - val_loss: 0.3280 - val_accuracy: 0.8955\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1745 - accuracy: 0.9269 - val_loss: 0.2579 - val_accuracy: 0.8856\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2018 - accuracy: 0.9112 - val_loss: 0.3077 - val_accuracy: 0.8955\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3213 - accuracy: 0.8799 - val_loss: 0.3058 - val_accuracy: 0.8806\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1668 - accuracy: 0.9385 - val_loss: 0.3585 - val_accuracy: 0.8159\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2366 - accuracy: 0.8958 - val_loss: 0.2780 - val_accuracy: 0.9055\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1978 - accuracy: 0.9137 - val_loss: 0.2389 - val_accuracy: 0.9204\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1467 - accuracy: 0.9323 - val_loss: 0.2207 - val_accuracy: 0.9005\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1564 - accuracy: 0.9355 - val_loss: 0.2208 - val_accuracy: 0.9204\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1393 - accuracy: 0.9473 - val_loss: 0.4091 - val_accuracy: 0.8358\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1521 - accuracy: 0.9290 - val_loss: 0.2624 - val_accuracy: 0.9005\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1360 - accuracy: 0.9335 - val_loss: 0.3222 - val_accuracy: 0.8657\n",
            "Score for fold 2: loss of 0.32221171259880066; accuracy of 86.56716346740723%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 6, 1, 32)     320         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 6, 1, 64)     18496       conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 6, 1, 64)     256         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 6, 1, 64)     0           conv2d_35[0][0]                  \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 6, 1, 64)     256         add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 6, 1, 64)     0           conv2d_35[0][0]                  \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 6, 1, 64)     256         add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 6, 1, 64)     0           conv2d_35[0][0]                  \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 6, 1, 64)     256         add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 6, 1, 64)     0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 6, 1, 64)     36928       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 6, 1, 64)     256         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 6, 1, 64)     0           conv2d_39[0][0]                  \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 6, 1, 64)     256         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 6, 1, 64)     0           conv2d_39[0][0]                  \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 6, 1, 64)     256         add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 6, 1, 64)     0           conv2d_39[0][0]                  \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 6, 1, 64)     256         add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 3, 1, 64)     0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 3, 1, 64)     0           max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 3, 1, 64)     36928       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 3, 1, 64)     256         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 3, 1, 64)     0           conv2d_43[0][0]                  \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 3, 1, 64)     256         add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 3, 1, 64)     0           conv2d_43[0][0]                  \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 3, 1, 64)     256         add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 3, 1, 64)     0           conv2d_43[0][0]                  \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 3, 1, 64)     256         add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 3, 1, 64)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 3, 1, 64)     36928       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 3, 1, 64)     256         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 3, 1, 64)     0           conv2d_47[0][0]                  \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 3, 1, 64)     256         add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 3, 1, 64)     0           conv2d_47[0][0]                  \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 3, 1, 64)     256         add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 3, 1, 64)     0           conv2d_47[0][0]                  \n",
            "                                                                 conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 3, 1, 64)     256         add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 2, 1, 64)     0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 128)          0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            258         flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 5s 13ms/step - loss: 0.9212 - accuracy: 0.5725 - val_loss: 0.5602 - val_accuracy: 0.7250\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6844 - accuracy: 0.6499 - val_loss: 0.6919 - val_accuracy: 0.6250\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5918 - accuracy: 0.7107 - val_loss: 0.5167 - val_accuracy: 0.7450\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5659 - accuracy: 0.7478 - val_loss: 0.5524 - val_accuracy: 0.7000\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4993 - accuracy: 0.8035 - val_loss: 0.5402 - val_accuracy: 0.6850\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5245 - accuracy: 0.7462 - val_loss: 0.4534 - val_accuracy: 0.7650\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.5103 - accuracy: 0.7661 - val_loss: 0.5425 - val_accuracy: 0.7400\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4807 - accuracy: 0.7803 - val_loss: 0.4111 - val_accuracy: 0.8250\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4854 - accuracy: 0.7694 - val_loss: 0.4377 - val_accuracy: 0.8150\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4376 - accuracy: 0.8156 - val_loss: 0.5518 - val_accuracy: 0.7500\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4927 - accuracy: 0.7684 - val_loss: 0.4393 - val_accuracy: 0.8200\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4613 - accuracy: 0.7939 - val_loss: 0.4039 - val_accuracy: 0.8400\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4534 - accuracy: 0.8045 - val_loss: 0.4586 - val_accuracy: 0.7850\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3781 - accuracy: 0.8173 - val_loss: 0.4235 - val_accuracy: 0.8200\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.3646 - accuracy: 0.8453 - val_loss: 0.3934 - val_accuracy: 0.8050\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3355 - accuracy: 0.8720 - val_loss: 0.4390 - val_accuracy: 0.8000\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3359 - accuracy: 0.8575 - val_loss: 0.4080 - val_accuracy: 0.8400\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3734 - accuracy: 0.8393 - val_loss: 0.4039 - val_accuracy: 0.8200\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.3550 - accuracy: 0.8524 - val_loss: 0.3802 - val_accuracy: 0.8400\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3155 - accuracy: 0.8744 - val_loss: 0.4474 - val_accuracy: 0.8000\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3686 - accuracy: 0.8456 - val_loss: 0.4387 - val_accuracy: 0.7800\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2981 - accuracy: 0.8711 - val_loss: 0.3519 - val_accuracy: 0.8400\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2988 - accuracy: 0.8805 - val_loss: 0.3584 - val_accuracy: 0.8250\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3288 - accuracy: 0.8617 - val_loss: 0.3609 - val_accuracy: 0.8350\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2563 - accuracy: 0.8962 - val_loss: 0.4219 - val_accuracy: 0.8150\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2447 - accuracy: 0.9130 - val_loss: 0.3287 - val_accuracy: 0.8600\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2963 - accuracy: 0.8966 - val_loss: 0.3730 - val_accuracy: 0.8250\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2738 - accuracy: 0.8893 - val_loss: 0.3287 - val_accuracy: 0.8650\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1948 - accuracy: 0.9123 - val_loss: 0.3419 - val_accuracy: 0.8600\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2550 - accuracy: 0.8873 - val_loss: 0.4340 - val_accuracy: 0.7900\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2135 - accuracy: 0.9077 - val_loss: 0.3019 - val_accuracy: 0.9000\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2621 - accuracy: 0.9049 - val_loss: 0.3387 - val_accuracy: 0.8400\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2732 - accuracy: 0.8941 - val_loss: 0.3547 - val_accuracy: 0.8400\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2334 - accuracy: 0.9003 - val_loss: 0.3544 - val_accuracy: 0.8450\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2276 - accuracy: 0.9085 - val_loss: 0.3474 - val_accuracy: 0.8500\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1832 - accuracy: 0.9334 - val_loss: 0.3565 - val_accuracy: 0.8700\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2111 - accuracy: 0.9269 - val_loss: 0.4193 - val_accuracy: 0.8350\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1705 - accuracy: 0.9316 - val_loss: 0.3795 - val_accuracy: 0.8150\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2322 - accuracy: 0.9172 - val_loss: 0.3084 - val_accuracy: 0.8700\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1757 - accuracy: 0.9299 - val_loss: 0.2860 - val_accuracy: 0.8900\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1271 - accuracy: 0.9528 - val_loss: 0.3235 - val_accuracy: 0.8800\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1666 - accuracy: 0.9357 - val_loss: 0.3299 - val_accuracy: 0.8850\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1966 - accuracy: 0.9293 - val_loss: 0.4393 - val_accuracy: 0.8100\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1720 - accuracy: 0.9309 - val_loss: 0.3095 - val_accuracy: 0.8700\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1692 - accuracy: 0.9433 - val_loss: 0.3495 - val_accuracy: 0.8650\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1635 - accuracy: 0.9493 - val_loss: 0.3265 - val_accuracy: 0.8700\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1325 - accuracy: 0.9463 - val_loss: 0.3582 - val_accuracy: 0.8900\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1544 - accuracy: 0.9379 - val_loss: 0.3043 - val_accuracy: 0.8500\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1663 - accuracy: 0.9460 - val_loss: 0.2967 - val_accuracy: 0.8900\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1264 - accuracy: 0.9581 - val_loss: 0.3239 - val_accuracy: 0.8850\n",
            "Score for fold 3: loss of 0.32387420535087585; accuracy of 88.49999904632568%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 6, 1, 32)     320         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 6, 1, 64)     18496       conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 6, 1, 64)     256         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_36 (Add)                    (None, 6, 1, 64)     0           conv2d_52[0][0]                  \n",
            "                                                                 conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 6, 1, 64)     256         add_36[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 6, 1, 64)     0           conv2d_52[0][0]                  \n",
            "                                                                 conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 6, 1, 64)     256         add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 6, 1, 64)     0           conv2d_52[0][0]                  \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 6, 1, 64)     256         add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 6, 1, 64)     0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 6, 1, 64)     36928       dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 6, 1, 64)     256         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 6, 1, 64)     0           conv2d_56[0][0]                  \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 6, 1, 64)     256         add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 6, 1, 64)     0           conv2d_56[0][0]                  \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 6, 1, 64)     256         add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 6, 1, 64)     0           conv2d_56[0][0]                  \n",
            "                                                                 conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 6, 1, 64)     256         add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 3, 1, 64)     0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 3, 1, 64)     36928       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 3, 1, 64)     256         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 3, 1, 64)     0           conv2d_60[0][0]                  \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 3, 1, 64)     256         add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 3, 1, 64)     0           conv2d_60[0][0]                  \n",
            "                                                                 conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 3, 1, 64)     256         add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 3, 1, 64)     0           conv2d_60[0][0]                  \n",
            "                                                                 conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 3, 1, 64)     256         add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 3, 1, 64)     36928       dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 3, 1, 64)     256         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 3, 1, 64)     0           conv2d_64[0][0]                  \n",
            "                                                                 conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 3, 1, 64)     256         add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 3, 1, 64)     0           conv2d_64[0][0]                  \n",
            "                                                                 conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 3, 1, 64)     256         add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 3, 1, 64)     0           conv2d_64[0][0]                  \n",
            "                                                                 conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 3, 1, 64)     256         add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 2, 1, 64)     0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 128)          0           max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            258         flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 4s 13ms/step - loss: 0.9466 - accuracy: 0.5546 - val_loss: 0.7975 - val_accuracy: 0.6250\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.7201 - accuracy: 0.6262 - val_loss: 0.7282 - val_accuracy: 0.6700\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5978 - accuracy: 0.7255 - val_loss: 0.9127 - val_accuracy: 0.5550\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5697 - accuracy: 0.7156 - val_loss: 0.7306 - val_accuracy: 0.6250\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5071 - accuracy: 0.7370 - val_loss: 0.6595 - val_accuracy: 0.7150\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4798 - accuracy: 0.7903 - val_loss: 0.4672 - val_accuracy: 0.7800\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4924 - accuracy: 0.8041 - val_loss: 0.5695 - val_accuracy: 0.7600\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4325 - accuracy: 0.8155 - val_loss: 0.5277 - val_accuracy: 0.7550\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4868 - accuracy: 0.7909 - val_loss: 0.5263 - val_accuracy: 0.7000\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4198 - accuracy: 0.8276 - val_loss: 0.5932 - val_accuracy: 0.7600\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4549 - accuracy: 0.7849 - val_loss: 0.5721 - val_accuracy: 0.7350\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4241 - accuracy: 0.8283 - val_loss: 0.5709 - val_accuracy: 0.7650\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4363 - accuracy: 0.8093 - val_loss: 0.4291 - val_accuracy: 0.7700\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4021 - accuracy: 0.8411 - val_loss: 0.3723 - val_accuracy: 0.8200\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3578 - accuracy: 0.8508 - val_loss: 0.3939 - val_accuracy: 0.8400\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3205 - accuracy: 0.8904 - val_loss: 0.3887 - val_accuracy: 0.7900\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.4266 - accuracy: 0.8329 - val_loss: 0.4082 - val_accuracy: 0.8200\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2875 - accuracy: 0.8852 - val_loss: 0.4337 - val_accuracy: 0.7950\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3176 - accuracy: 0.8653 - val_loss: 0.4259 - val_accuracy: 0.8050\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3291 - accuracy: 0.8602 - val_loss: 0.3463 - val_accuracy: 0.8250\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2791 - accuracy: 0.8926 - val_loss: 0.4073 - val_accuracy: 0.8100\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2972 - accuracy: 0.8925 - val_loss: 0.3658 - val_accuracy: 0.8500\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2596 - accuracy: 0.8895 - val_loss: 0.3379 - val_accuracy: 0.8850\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2615 - accuracy: 0.8864 - val_loss: 0.2746 - val_accuracy: 0.8950\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3056 - accuracy: 0.8695 - val_loss: 0.3963 - val_accuracy: 0.8300\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2905 - accuracy: 0.8854 - val_loss: 0.3359 - val_accuracy: 0.8450\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2112 - accuracy: 0.9227 - val_loss: 0.3456 - val_accuracy: 0.8350\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2722 - accuracy: 0.8970 - val_loss: 0.3020 - val_accuracy: 0.8300\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2833 - accuracy: 0.8696 - val_loss: 0.3379 - val_accuracy: 0.8500\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2961 - accuracy: 0.8910 - val_loss: 0.4168 - val_accuracy: 0.8150\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2645 - accuracy: 0.8909 - val_loss: 0.3188 - val_accuracy: 0.8400\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2156 - accuracy: 0.9229 - val_loss: 0.2834 - val_accuracy: 0.8650\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2037 - accuracy: 0.9195 - val_loss: 0.3361 - val_accuracy: 0.8650\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2106 - accuracy: 0.9299 - val_loss: 0.2875 - val_accuracy: 0.8750\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1848 - accuracy: 0.9373 - val_loss: 0.3750 - val_accuracy: 0.8350\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2633 - accuracy: 0.8942 - val_loss: 0.3015 - val_accuracy: 0.8700\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1852 - accuracy: 0.9337 - val_loss: 0.3428 - val_accuracy: 0.8400\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2047 - accuracy: 0.9346 - val_loss: 0.3376 - val_accuracy: 0.8500\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1868 - accuracy: 0.9343 - val_loss: 0.3177 - val_accuracy: 0.8500\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1409 - accuracy: 0.9623 - val_loss: 0.2996 - val_accuracy: 0.8550\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1756 - accuracy: 0.9406 - val_loss: 0.3540 - val_accuracy: 0.8450\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1937 - accuracy: 0.9302 - val_loss: 0.3326 - val_accuracy: 0.8500\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1848 - accuracy: 0.9345 - val_loss: 0.3872 - val_accuracy: 0.8700\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1556 - accuracy: 0.9474 - val_loss: 0.3307 - val_accuracy: 0.8500\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1620 - accuracy: 0.9396 - val_loss: 0.2823 - val_accuracy: 0.8950\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1398 - accuracy: 0.9554 - val_loss: 0.3478 - val_accuracy: 0.8450\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2006 - accuracy: 0.9324 - val_loss: 0.3516 - val_accuracy: 0.8300\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1671 - accuracy: 0.9363 - val_loss: 0.3542 - val_accuracy: 0.8550\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1114 - accuracy: 0.9639 - val_loss: 0.4056 - val_accuracy: 0.8400\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1427 - accuracy: 0.9482 - val_loss: 0.3487 - val_accuracy: 0.8450\n",
            "Score for fold 4: loss of 0.3487468361854553; accuracy of 84.50000286102295%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 6, 1, 32)     320         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 6, 1, 64)     18496       conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 6, 1, 64)     256         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 6, 1, 64)     0           conv2d_69[0][0]                  \n",
            "                                                                 conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 6, 1, 64)     256         add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 6, 1, 64)     0           conv2d_69[0][0]                  \n",
            "                                                                 conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 6, 1, 64)     256         add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 6, 1, 64)     0           conv2d_69[0][0]                  \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 6, 1, 64)     256         add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 6, 1, 64)     36928       dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 6, 1, 64)     256         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 6, 1, 64)     0           conv2d_73[0][0]                  \n",
            "                                                                 conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 6, 1, 64)     256         add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 6, 1, 64)     0           conv2d_73[0][0]                  \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 6, 1, 64)     256         add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 6, 1, 64)     0           conv2d_73[0][0]                  \n",
            "                                                                 conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 6, 1, 64)     256         add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 3, 1, 64)     0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 3, 1, 64)     36928       dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 3, 1, 64)     256         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 3, 1, 64)     0           conv2d_77[0][0]                  \n",
            "                                                                 conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 3, 1, 64)     256         add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_55 (Add)                    (None, 3, 1, 64)     0           conv2d_77[0][0]                  \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 3, 1, 64)     256         add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_56 (Add)                    (None, 3, 1, 64)     0           conv2d_77[0][0]                  \n",
            "                                                                 conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 3, 1, 64)     256         add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 3, 1, 64)     36928       dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 3, 1, 64)     256         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_57 (Add)                    (None, 3, 1, 64)     0           conv2d_81[0][0]                  \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 3, 1, 64)     256         add_57[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_58 (Add)                    (None, 3, 1, 64)     0           conv2d_81[0][0]                  \n",
            "                                                                 conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 3, 1, 64)     256         add_58[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_59 (Add)                    (None, 3, 1, 64)     0           conv2d_81[0][0]                  \n",
            "                                                                 conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 3, 1, 64)     256         add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 2, 1, 64)     0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 128)          0           max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 2)            258         flatten_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 5s 13ms/step - loss: 0.8813 - accuracy: 0.5961 - val_loss: 0.6623 - val_accuracy: 0.6200\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6595 - accuracy: 0.6961 - val_loss: 0.6117 - val_accuracy: 0.6900\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.6157 - accuracy: 0.7053 - val_loss: 0.5885 - val_accuracy: 0.7500\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.5450 - accuracy: 0.7446 - val_loss: 0.5993 - val_accuracy: 0.7200\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.6005 - accuracy: 0.7007 - val_loss: 0.4927 - val_accuracy: 0.8100\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4520 - accuracy: 0.7925 - val_loss: 0.5967 - val_accuracy: 0.7450\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 3s 12ms/step - loss: 0.4937 - accuracy: 0.7566 - val_loss: 0.4797 - val_accuracy: 0.7700\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4485 - accuracy: 0.8066 - val_loss: 0.5237 - val_accuracy: 0.7950\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4450 - accuracy: 0.7949 - val_loss: 0.4978 - val_accuracy: 0.8000\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4464 - accuracy: 0.8122 - val_loss: 0.4995 - val_accuracy: 0.7950\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4319 - accuracy: 0.8116 - val_loss: 0.4546 - val_accuracy: 0.7950\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4071 - accuracy: 0.8139 - val_loss: 0.4477 - val_accuracy: 0.8350\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.3833 - accuracy: 0.8495 - val_loss: 0.5266 - val_accuracy: 0.7350\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.4363 - accuracy: 0.7975 - val_loss: 0.4943 - val_accuracy: 0.7400\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3679 - accuracy: 0.8574 - val_loss: 0.4474 - val_accuracy: 0.8100\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3228 - accuracy: 0.8626 - val_loss: 0.4494 - val_accuracy: 0.7550\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.3205 - accuracy: 0.8579 - val_loss: 0.5728 - val_accuracy: 0.7600\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.3661 - accuracy: 0.8336 - val_loss: 0.3791 - val_accuracy: 0.8450\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2917 - accuracy: 0.8755 - val_loss: 0.4479 - val_accuracy: 0.8100\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2894 - accuracy: 0.8657 - val_loss: 0.3718 - val_accuracy: 0.8450\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.3042 - accuracy: 0.8805 - val_loss: 0.4082 - val_accuracy: 0.8150\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2908 - accuracy: 0.8837 - val_loss: 0.4809 - val_accuracy: 0.8050\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2832 - accuracy: 0.8961 - val_loss: 0.4887 - val_accuracy: 0.8000\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2640 - accuracy: 0.9075 - val_loss: 0.3619 - val_accuracy: 0.8300\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2575 - accuracy: 0.8833 - val_loss: 0.3525 - val_accuracy: 0.8300\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2631 - accuracy: 0.9066 - val_loss: 0.4001 - val_accuracy: 0.8350\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2252 - accuracy: 0.9051 - val_loss: 0.3575 - val_accuracy: 0.8650\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2434 - accuracy: 0.9088 - val_loss: 0.4364 - val_accuracy: 0.8350\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2546 - accuracy: 0.8945 - val_loss: 0.4060 - val_accuracy: 0.8150\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2342 - accuracy: 0.9118 - val_loss: 0.3840 - val_accuracy: 0.8250\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2322 - accuracy: 0.9069 - val_loss: 0.4113 - val_accuracy: 0.8400\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2343 - accuracy: 0.8888 - val_loss: 0.4000 - val_accuracy: 0.8150\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2124 - accuracy: 0.9011 - val_loss: 0.3614 - val_accuracy: 0.8350\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1504 - accuracy: 0.9395 - val_loss: 0.3924 - val_accuracy: 0.8400\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2112 - accuracy: 0.9164 - val_loss: 0.3104 - val_accuracy: 0.8900\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1828 - accuracy: 0.9371 - val_loss: 0.4020 - val_accuracy: 0.8500\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1876 - accuracy: 0.9300 - val_loss: 0.3908 - val_accuracy: 0.8500\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1680 - accuracy: 0.9347 - val_loss: 0.3674 - val_accuracy: 0.8700\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1881 - accuracy: 0.9246 - val_loss: 0.3841 - val_accuracy: 0.8450\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.2030 - accuracy: 0.9193 - val_loss: 0.3306 - val_accuracy: 0.8550\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1688 - accuracy: 0.9355 - val_loss: 0.2618 - val_accuracy: 0.8900\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1510 - accuracy: 0.9358 - val_loss: 0.3186 - val_accuracy: 0.8950\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1443 - accuracy: 0.9497 - val_loss: 0.3247 - val_accuracy: 0.8800\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1630 - accuracy: 0.9423 - val_loss: 0.3124 - val_accuracy: 0.8750\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1439 - accuracy: 0.9391 - val_loss: 0.2862 - val_accuracy: 0.8950\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1428 - accuracy: 0.9432 - val_loss: 0.3052 - val_accuracy: 0.8650\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1178 - accuracy: 0.9582 - val_loss: 0.3581 - val_accuracy: 0.8500\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.0985 - accuracy: 0.9635 - val_loss: 0.3878 - val_accuracy: 0.8600\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1627 - accuracy: 0.9414 - val_loss: 0.3505 - val_accuracy: 0.8950\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.1248 - accuracy: 0.9531 - val_loss: 0.3144 - val_accuracy: 0.8750\n",
            "Score for fold 5: loss of 0.31436431407928467; accuracy of 87.5%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.3204331398010254 - Accuracy: 85.57214140892029%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.32221171259880066 - Accuracy: 86.56716346740723%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.32387420535087585 - Accuracy: 88.49999904632568%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.3487468361854553 - Accuracy: 84.50000286102295%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.31436431407928467 - Accuracy: 87.5%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 86.52786135673523 (+- 1.40438854749097)\n",
            "> Loss: 0.32592604160308836\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 6, 1, 32)     320         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 6, 1, 64)     18496       conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 6, 1, 64)     256         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_60 (Add)                    (None, 6, 1, 64)     0           conv2d_86[0][0]                  \n",
            "                                                                 conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 6, 1, 64)     256         add_60[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_61 (Add)                    (None, 6, 1, 64)     0           conv2d_86[0][0]                  \n",
            "                                                                 conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 6, 1, 64)     256         add_61[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_62 (Add)                    (None, 6, 1, 64)     0           conv2d_86[0][0]                  \n",
            "                                                                 conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 6, 1, 64)     256         add_62[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 6, 1, 64)     36928       dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 6, 1, 64)     256         conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_63 (Add)                    (None, 6, 1, 64)     0           conv2d_90[0][0]                  \n",
            "                                                                 conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 6, 1, 64)     256         add_63[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_64 (Add)                    (None, 6, 1, 64)     0           conv2d_90[0][0]                  \n",
            "                                                                 conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 6, 1, 64)     256         add_64[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 6, 1, 64)     36928       batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_65 (Add)                    (None, 6, 1, 64)     0           conv2d_90[0][0]                  \n",
            "                                                                 conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 6, 1, 64)     256         add_65[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 3, 1, 64)     36928       dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 3, 1, 64)     256         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_66 (Add)                    (None, 3, 1, 64)     0           conv2d_94[0][0]                  \n",
            "                                                                 conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 3, 1, 64)     256         add_66[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_67 (Add)                    (None, 3, 1, 64)     0           conv2d_94[0][0]                  \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 3, 1, 64)     256         add_67[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_68 (Add)                    (None, 3, 1, 64)     0           conv2d_94[0][0]                  \n",
            "                                                                 conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 3, 1, 64)     256         add_68[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 3, 1, 64)     36928       dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 3, 1, 64)     256         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 3, 1, 64)     36928       batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_69 (Add)                    (None, 3, 1, 64)     0           conv2d_98[0][0]                  \n",
            "                                                                 conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 3, 1, 64)     256         add_69[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_70 (Add)                    (None, 3, 1, 64)     0           conv2d_98[0][0]                  \n",
            "                                                                 conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 3, 1, 64)     256         add_70[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_71 (Add)                    (None, 3, 1, 64)     0           conv2d_98[0][0]                  \n",
            "                                                                 conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 3, 1, 64)     256         add_71[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 128)          0           max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 2)            258         flatten_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 5s 16ms/step - loss: 0.8929 - accuracy: 0.6165 - val_loss: 0.6902 - val_accuracy: 0.6617\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.7487 - accuracy: 0.6195 - val_loss: 0.6587 - val_accuracy: 0.6965\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.6798 - accuracy: 0.6407 - val_loss: 0.6438 - val_accuracy: 0.6716\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.6585 - accuracy: 0.6887 - val_loss: 0.5710 - val_accuracy: 0.7612\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5855 - accuracy: 0.7040 - val_loss: 0.5191 - val_accuracy: 0.7612\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5370 - accuracy: 0.7388 - val_loss: 0.4770 - val_accuracy: 0.7910\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5395 - accuracy: 0.7521 - val_loss: 0.4907 - val_accuracy: 0.7463\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5182 - accuracy: 0.7603 - val_loss: 0.4953 - val_accuracy: 0.7662\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4659 - accuracy: 0.7857 - val_loss: 0.4847 - val_accuracy: 0.7761\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5030 - accuracy: 0.7740 - val_loss: 0.4655 - val_accuracy: 0.8010\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4498 - accuracy: 0.7981 - val_loss: 0.4825 - val_accuracy: 0.7960\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4706 - accuracy: 0.7894 - val_loss: 0.4556 - val_accuracy: 0.8259\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4664 - accuracy: 0.8151 - val_loss: 0.4327 - val_accuracy: 0.8060\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3963 - accuracy: 0.8461 - val_loss: 0.4967 - val_accuracy: 0.7861\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4580 - accuracy: 0.7876 - val_loss: 0.5233 - val_accuracy: 0.7612\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4639 - accuracy: 0.7977 - val_loss: 0.4670 - val_accuracy: 0.8010\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4195 - accuracy: 0.8202 - val_loss: 0.4867 - val_accuracy: 0.8159\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4229 - accuracy: 0.8035 - val_loss: 0.4703 - val_accuracy: 0.7662\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4594 - accuracy: 0.8081 - val_loss: 0.4766 - val_accuracy: 0.8109\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4047 - accuracy: 0.8277 - val_loss: 0.5324 - val_accuracy: 0.7711\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4173 - accuracy: 0.8205 - val_loss: 0.4574 - val_accuracy: 0.8159\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3887 - accuracy: 0.8377 - val_loss: 0.5103 - val_accuracy: 0.7811\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4832 - accuracy: 0.7885 - val_loss: 0.4322 - val_accuracy: 0.8060\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3413 - accuracy: 0.8715 - val_loss: 0.4190 - val_accuracy: 0.8259\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3757 - accuracy: 0.8377 - val_loss: 0.4398 - val_accuracy: 0.8010\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3435 - accuracy: 0.8451 - val_loss: 0.4315 - val_accuracy: 0.7960\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3869 - accuracy: 0.8274 - val_loss: 0.4326 - val_accuracy: 0.8458\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3486 - accuracy: 0.8512 - val_loss: 0.4048 - val_accuracy: 0.8209\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3459 - accuracy: 0.8498 - val_loss: 0.3963 - val_accuracy: 0.8657\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3457 - accuracy: 0.8774 - val_loss: 0.3593 - val_accuracy: 0.8557\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3483 - accuracy: 0.8564 - val_loss: 0.4830 - val_accuracy: 0.8259\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3276 - accuracy: 0.8680 - val_loss: 0.3946 - val_accuracy: 0.8259\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3299 - accuracy: 0.8675 - val_loss: 0.3807 - val_accuracy: 0.8259\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3273 - accuracy: 0.8544 - val_loss: 0.4076 - val_accuracy: 0.8408\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2934 - accuracy: 0.8752 - val_loss: 0.3572 - val_accuracy: 0.8557\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2995 - accuracy: 0.8712 - val_loss: 0.3895 - val_accuracy: 0.8607\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3259 - accuracy: 0.8662 - val_loss: 0.3750 - val_accuracy: 0.8507\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2685 - accuracy: 0.8969 - val_loss: 0.3867 - val_accuracy: 0.8507\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3012 - accuracy: 0.8791 - val_loss: 0.3821 - val_accuracy: 0.8458\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 3s 13ms/step - loss: 0.2992 - accuracy: 0.8746 - val_loss: 0.3423 - val_accuracy: 0.8557\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3045 - accuracy: 0.8725 - val_loss: 0.4044 - val_accuracy: 0.8507\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 3s 13ms/step - loss: 0.2861 - accuracy: 0.8800 - val_loss: 0.3903 - val_accuracy: 0.8358\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 3s 13ms/step - loss: 0.2717 - accuracy: 0.8835 - val_loss: 0.3914 - val_accuracy: 0.8060\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 3s 13ms/step - loss: 0.2627 - accuracy: 0.8973 - val_loss: 0.3958 - val_accuracy: 0.8308\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3110 - accuracy: 0.8793 - val_loss: 0.3791 - val_accuracy: 0.8458\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 3s 13ms/step - loss: 0.2666 - accuracy: 0.8830 - val_loss: 0.3540 - val_accuracy: 0.8209\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2263 - accuracy: 0.9179 - val_loss: 0.3562 - val_accuracy: 0.8806\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 3s 13ms/step - loss: 0.3011 - accuracy: 0.8855 - val_loss: 0.3372 - val_accuracy: 0.8557\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2103 - accuracy: 0.9040 - val_loss: 0.4031 - val_accuracy: 0.8458\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2590 - accuracy: 0.8928 - val_loss: 0.3615 - val_accuracy: 0.8657\n",
            "Score for fold 1: loss of 0.3615168035030365; accuracy of 86.56716346740723%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 6, 1, 32)     320         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 6, 1, 64)     18496       conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 6, 1, 64)     256         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_72 (Add)                    (None, 6, 1, 64)     0           conv2d_103[0][0]                 \n",
            "                                                                 conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 6, 1, 64)     256         add_72[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_73 (Add)                    (None, 6, 1, 64)     0           conv2d_103[0][0]                 \n",
            "                                                                 conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 6, 1, 64)     256         add_73[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_74 (Add)                    (None, 6, 1, 64)     0           conv2d_103[0][0]                 \n",
            "                                                                 conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 6, 1, 64)     256         add_74[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 6, 1, 64)     36928       dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 6, 1, 64)     256         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_75 (Add)                    (None, 6, 1, 64)     0           conv2d_107[0][0]                 \n",
            "                                                                 conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 6, 1, 64)     256         add_75[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_76 (Add)                    (None, 6, 1, 64)     0           conv2d_107[0][0]                 \n",
            "                                                                 conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 6, 1, 64)     256         add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_77 (Add)                    (None, 6, 1, 64)     0           conv2d_107[0][0]                 \n",
            "                                                                 conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 6, 1, 64)     256         add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 3, 1, 64)     36928       dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 3, 1, 64)     256         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_78 (Add)                    (None, 3, 1, 64)     0           conv2d_111[0][0]                 \n",
            "                                                                 conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 3, 1, 64)     256         add_78[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_79 (Add)                    (None, 3, 1, 64)     0           conv2d_111[0][0]                 \n",
            "                                                                 conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 3, 1, 64)     256         add_79[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_80 (Add)                    (None, 3, 1, 64)     0           conv2d_111[0][0]                 \n",
            "                                                                 conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 3, 1, 64)     256         add_80[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 3, 1, 64)     36928       dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 3, 1, 64)     256         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_81 (Add)                    (None, 3, 1, 64)     0           conv2d_115[0][0]                 \n",
            "                                                                 conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 3, 1, 64)     256         add_81[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_82 (Add)                    (None, 3, 1, 64)     0           conv2d_115[0][0]                 \n",
            "                                                                 conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 3, 1, 64)     256         add_82[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_83 (Add)                    (None, 3, 1, 64)     0           conv2d_115[0][0]                 \n",
            "                                                                 conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 3, 1, 64)     256         add_83[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 128)          0           max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 2)            258         flatten_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 5s 16ms/step - loss: 0.8523 - accuracy: 0.6073 - val_loss: 0.7466 - val_accuracy: 0.5771\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.7446 - accuracy: 0.6078 - val_loss: 0.6387 - val_accuracy: 0.6368\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.6382 - accuracy: 0.6453 - val_loss: 0.6372 - val_accuracy: 0.6965\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5979 - accuracy: 0.7124 - val_loss: 0.5873 - val_accuracy: 0.6965\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5927 - accuracy: 0.6915 - val_loss: 0.6154 - val_accuracy: 0.7214\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5691 - accuracy: 0.7472 - val_loss: 0.5109 - val_accuracy: 0.7363\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5277 - accuracy: 0.7396 - val_loss: 0.5934 - val_accuracy: 0.7214\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5352 - accuracy: 0.7357 - val_loss: 0.6005 - val_accuracy: 0.7512\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5026 - accuracy: 0.7786 - val_loss: 0.5763 - val_accuracy: 0.7264\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4441 - accuracy: 0.8093 - val_loss: 0.5609 - val_accuracy: 0.7114\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4334 - accuracy: 0.7982 - val_loss: 0.5442 - val_accuracy: 0.7662\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4619 - accuracy: 0.7991 - val_loss: 0.5192 - val_accuracy: 0.7264\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4583 - accuracy: 0.8083 - val_loss: 0.5234 - val_accuracy: 0.7463\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4529 - accuracy: 0.8105 - val_loss: 0.5592 - val_accuracy: 0.7463\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4673 - accuracy: 0.7880 - val_loss: 0.5451 - val_accuracy: 0.7363\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3775 - accuracy: 0.8540 - val_loss: 0.5108 - val_accuracy: 0.7463\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4161 - accuracy: 0.8332 - val_loss: 0.5144 - val_accuracy: 0.7413\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4088 - accuracy: 0.8061 - val_loss: 0.5005 - val_accuracy: 0.7811\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4002 - accuracy: 0.8129 - val_loss: 0.6180 - val_accuracy: 0.7264\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4020 - accuracy: 0.8037 - val_loss: 0.4852 - val_accuracy: 0.7562\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3621 - accuracy: 0.8640 - val_loss: 0.5977 - val_accuracy: 0.7313\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3962 - accuracy: 0.8313 - val_loss: 0.5661 - val_accuracy: 0.7562\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3784 - accuracy: 0.8393 - val_loss: 0.5259 - val_accuracy: 0.7562\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3486 - accuracy: 0.8552 - val_loss: 0.5365 - val_accuracy: 0.7711\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3551 - accuracy: 0.8365 - val_loss: 0.5006 - val_accuracy: 0.7761\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3627 - accuracy: 0.8541 - val_loss: 0.4963 - val_accuracy: 0.7960\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3717 - accuracy: 0.8306 - val_loss: 0.4862 - val_accuracy: 0.7960\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3674 - accuracy: 0.8500 - val_loss: 0.5475 - val_accuracy: 0.7662\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3642 - accuracy: 0.8476 - val_loss: 0.5023 - val_accuracy: 0.7761\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3064 - accuracy: 0.8859 - val_loss: 0.5264 - val_accuracy: 0.7861\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3284 - accuracy: 0.8738 - val_loss: 0.4934 - val_accuracy: 0.7861\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3144 - accuracy: 0.8701 - val_loss: 0.5450 - val_accuracy: 0.7562\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3392 - accuracy: 0.8521 - val_loss: 0.5198 - val_accuracy: 0.7612\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3181 - accuracy: 0.8777 - val_loss: 0.4658 - val_accuracy: 0.8010\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3151 - accuracy: 0.8751 - val_loss: 0.4699 - val_accuracy: 0.7811\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2886 - accuracy: 0.8843 - val_loss: 0.6189 - val_accuracy: 0.7114\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4604 - accuracy: 0.7995 - val_loss: 0.5156 - val_accuracy: 0.7065\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3613 - accuracy: 0.8406 - val_loss: 0.4688 - val_accuracy: 0.7861\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3060 - accuracy: 0.8871 - val_loss: 0.4344 - val_accuracy: 0.7960\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2998 - accuracy: 0.8690 - val_loss: 0.4974 - val_accuracy: 0.8010\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3217 - accuracy: 0.8488 - val_loss: 0.4154 - val_accuracy: 0.8159\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2718 - accuracy: 0.8821 - val_loss: 0.4325 - val_accuracy: 0.7910\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2784 - accuracy: 0.8979 - val_loss: 0.4361 - val_accuracy: 0.8259\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3147 - accuracy: 0.8838 - val_loss: 0.4562 - val_accuracy: 0.8060\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2573 - accuracy: 0.8951 - val_loss: 0.4229 - val_accuracy: 0.7960\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2127 - accuracy: 0.9117 - val_loss: 0.4437 - val_accuracy: 0.8358\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2860 - accuracy: 0.8937 - val_loss: 0.4535 - val_accuracy: 0.7761\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2395 - accuracy: 0.9164 - val_loss: 0.4549 - val_accuracy: 0.8259\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2418 - accuracy: 0.8962 - val_loss: 0.4979 - val_accuracy: 0.7811\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2761 - accuracy: 0.8798 - val_loss: 0.4244 - val_accuracy: 0.8209\n",
            "Score for fold 2: loss of 0.4244372248649597; accuracy of 82.08954930305481%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 6, 1, 32)     320         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 6, 1, 64)     18496       conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 6, 1, 64)     256         conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_84 (Add)                    (None, 6, 1, 64)     0           conv2d_120[0][0]                 \n",
            "                                                                 conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 6, 1, 64)     256         add_84[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_85 (Add)                    (None, 6, 1, 64)     0           conv2d_120[0][0]                 \n",
            "                                                                 conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 6, 1, 64)     256         add_85[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_86 (Add)                    (None, 6, 1, 64)     0           conv2d_120[0][0]                 \n",
            "                                                                 conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 6, 1, 64)     256         add_86[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 6, 1, 64)     36928       dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 6, 1, 64)     256         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_87 (Add)                    (None, 6, 1, 64)     0           conv2d_124[0][0]                 \n",
            "                                                                 conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 6, 1, 64)     256         add_87[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_88 (Add)                    (None, 6, 1, 64)     0           conv2d_124[0][0]                 \n",
            "                                                                 conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 6, 1, 64)     256         add_88[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_89 (Add)                    (None, 6, 1, 64)     0           conv2d_124[0][0]                 \n",
            "                                                                 conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 6, 1, 64)     256         add_89[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 3, 1, 64)     36928       dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 3, 1, 64)     256         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_90 (Add)                    (None, 3, 1, 64)     0           conv2d_128[0][0]                 \n",
            "                                                                 conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 3, 1, 64)     256         add_90[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_91 (Add)                    (None, 3, 1, 64)     0           conv2d_128[0][0]                 \n",
            "                                                                 conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 3, 1, 64)     256         add_91[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_92 (Add)                    (None, 3, 1, 64)     0           conv2d_128[0][0]                 \n",
            "                                                                 conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 3, 1, 64)     256         add_92[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 3, 1, 64)     36928       dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 3, 1, 64)     256         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_93 (Add)                    (None, 3, 1, 64)     0           conv2d_132[0][0]                 \n",
            "                                                                 conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 3, 1, 64)     256         add_93[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_94 (Add)                    (None, 3, 1, 64)     0           conv2d_132[0][0]                 \n",
            "                                                                 conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 3, 1, 64)     256         add_94[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_95 (Add)                    (None, 3, 1, 64)     0           conv2d_132[0][0]                 \n",
            "                                                                 conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 3, 1, 64)     256         add_95[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 128)          0           max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 2)            258         flatten_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 5s 17ms/step - loss: 0.9747 - accuracy: 0.5669 - val_loss: 0.6828 - val_accuracy: 0.6600\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.6966 - accuracy: 0.6656 - val_loss: 0.7506 - val_accuracy: 0.6450\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5986 - accuracy: 0.7164 - val_loss: 0.8451 - val_accuracy: 0.6750\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.6344 - accuracy: 0.6920 - val_loss: 0.5311 - val_accuracy: 0.7500\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5792 - accuracy: 0.7561 - val_loss: 0.5588 - val_accuracy: 0.7200\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5616 - accuracy: 0.7419 - val_loss: 0.5327 - val_accuracy: 0.7550\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5066 - accuracy: 0.7656 - val_loss: 0.5770 - val_accuracy: 0.7850\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5292 - accuracy: 0.7600 - val_loss: 0.5789 - val_accuracy: 0.7300\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5140 - accuracy: 0.7473 - val_loss: 0.5084 - val_accuracy: 0.8000\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.5009 - accuracy: 0.7672 - val_loss: 0.5305 - val_accuracy: 0.7300\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4912 - accuracy: 0.7559 - val_loss: 0.4921 - val_accuracy: 0.7700\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4922 - accuracy: 0.7729 - val_loss: 0.4742 - val_accuracy: 0.7800\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5029 - accuracy: 0.7422 - val_loss: 0.5186 - val_accuracy: 0.7450\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4624 - accuracy: 0.7934 - val_loss: 0.4901 - val_accuracy: 0.7850\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4920 - accuracy: 0.7944 - val_loss: 0.4510 - val_accuracy: 0.7750\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4581 - accuracy: 0.7870 - val_loss: 0.4528 - val_accuracy: 0.7800\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4638 - accuracy: 0.7988 - val_loss: 0.4718 - val_accuracy: 0.7600\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4432 - accuracy: 0.7859 - val_loss: 0.5085 - val_accuracy: 0.7950\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4350 - accuracy: 0.7926 - val_loss: 0.4504 - val_accuracy: 0.7950\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4629 - accuracy: 0.7997 - val_loss: 0.4045 - val_accuracy: 0.8150\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3789 - accuracy: 0.8520 - val_loss: 0.5315 - val_accuracy: 0.7350\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4304 - accuracy: 0.8100 - val_loss: 0.4659 - val_accuracy: 0.8100\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4059 - accuracy: 0.8285 - val_loss: 0.4391 - val_accuracy: 0.8100\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.4018 - accuracy: 0.8220 - val_loss: 0.4226 - val_accuracy: 0.8050\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3924 - accuracy: 0.7975 - val_loss: 0.4171 - val_accuracy: 0.8150\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3853 - accuracy: 0.8322 - val_loss: 0.4490 - val_accuracy: 0.7850\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3619 - accuracy: 0.8431 - val_loss: 0.3923 - val_accuracy: 0.8150\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3868 - accuracy: 0.8381 - val_loss: 0.4087 - val_accuracy: 0.8400\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3269 - accuracy: 0.8511 - val_loss: 0.4226 - val_accuracy: 0.7800\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3393 - accuracy: 0.8579 - val_loss: 0.4392 - val_accuracy: 0.7900\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3658 - accuracy: 0.8349 - val_loss: 0.3356 - val_accuracy: 0.8550\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2958 - accuracy: 0.8847 - val_loss: 0.4322 - val_accuracy: 0.7900\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3141 - accuracy: 0.8672 - val_loss: 0.5046 - val_accuracy: 0.8050\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3408 - accuracy: 0.8552 - val_loss: 0.3369 - val_accuracy: 0.8550\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3519 - accuracy: 0.8526 - val_loss: 0.3850 - val_accuracy: 0.8350\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3841 - accuracy: 0.8204 - val_loss: 0.4253 - val_accuracy: 0.8200\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2806 - accuracy: 0.8888 - val_loss: 0.3875 - val_accuracy: 0.8150\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2651 - accuracy: 0.9007 - val_loss: 0.3922 - val_accuracy: 0.8200\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3002 - accuracy: 0.8677 - val_loss: 0.4421 - val_accuracy: 0.8050\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2813 - accuracy: 0.8800 - val_loss: 0.4387 - val_accuracy: 0.7900\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3207 - accuracy: 0.8759 - val_loss: 0.4544 - val_accuracy: 0.7950\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2836 - accuracy: 0.8815 - val_loss: 0.3556 - val_accuracy: 0.8250\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.3071 - accuracy: 0.8761 - val_loss: 0.4149 - val_accuracy: 0.8000\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2222 - accuracy: 0.9056 - val_loss: 0.3806 - val_accuracy: 0.8400\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2850 - accuracy: 0.8828 - val_loss: 0.3242 - val_accuracy: 0.8550\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2336 - accuracy: 0.9062 - val_loss: 0.3400 - val_accuracy: 0.8300\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2787 - accuracy: 0.8950 - val_loss: 0.4315 - val_accuracy: 0.7750\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2073 - accuracy: 0.9109 - val_loss: 0.3998 - val_accuracy: 0.8000\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3081 - accuracy: 0.8687 - val_loss: 0.3323 - val_accuracy: 0.8400\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2451 - accuracy: 0.8863 - val_loss: 0.3408 - val_accuracy: 0.8200\n",
            "Score for fold 3: loss of 0.34077614545822144; accuracy of 81.99999928474426%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 6, 1, 32)     320         input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 6, 1, 64)     18496       conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 6, 1, 64)     256         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_96 (Add)                    (None, 6, 1, 64)     0           conv2d_137[0][0]                 \n",
            "                                                                 conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 6, 1, 64)     256         add_96[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_97 (Add)                    (None, 6, 1, 64)     0           conv2d_137[0][0]                 \n",
            "                                                                 conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 6, 1, 64)     256         add_97[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_98 (Add)                    (None, 6, 1, 64)     0           conv2d_137[0][0]                 \n",
            "                                                                 conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 6, 1, 64)     256         add_98[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 6, 1, 64)     36928       dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 6, 1, 64)     256         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_99 (Add)                    (None, 6, 1, 64)     0           conv2d_141[0][0]                 \n",
            "                                                                 conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 6, 1, 64)     256         add_99[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_100 (Add)                   (None, 6, 1, 64)     0           conv2d_141[0][0]                 \n",
            "                                                                 conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 6, 1, 64)     256         add_100[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_101 (Add)                   (None, 6, 1, 64)     0           conv2d_141[0][0]                 \n",
            "                                                                 conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 6, 1, 64)     256         add_101[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 3, 1, 64)     36928       dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 3, 1, 64)     256         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_102 (Add)                   (None, 3, 1, 64)     0           conv2d_145[0][0]                 \n",
            "                                                                 conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 3, 1, 64)     256         add_102[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_103 (Add)                   (None, 3, 1, 64)     0           conv2d_145[0][0]                 \n",
            "                                                                 conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 3, 1, 64)     256         add_103[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_104 (Add)                   (None, 3, 1, 64)     0           conv2d_145[0][0]                 \n",
            "                                                                 conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 3, 1, 64)     256         add_104[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 3, 1, 64)     36928       dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 3, 1, 64)     256         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_105 (Add)                   (None, 3, 1, 64)     0           conv2d_149[0][0]                 \n",
            "                                                                 conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 3, 1, 64)     256         add_105[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_106 (Add)                   (None, 3, 1, 64)     0           conv2d_149[0][0]                 \n",
            "                                                                 conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 3, 1, 64)     256         add_106[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_107 (Add)                   (None, 3, 1, 64)     0           conv2d_149[0][0]                 \n",
            "                                                                 conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 3, 1, 64)     256         add_107[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_8 (Flatten)             (None, 128)          0           max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 2)            258         flatten_8[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 6s 17ms/step - loss: 0.9393 - accuracy: 0.5702 - val_loss: 0.6846 - val_accuracy: 0.7350\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.7275 - accuracy: 0.6459 - val_loss: 0.7105 - val_accuracy: 0.6750\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.6050 - accuracy: 0.7018 - val_loss: 0.7769 - val_accuracy: 0.7350\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5487 - accuracy: 0.7533 - val_loss: 0.5750 - val_accuracy: 0.7350\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5470 - accuracy: 0.7401 - val_loss: 0.6187 - val_accuracy: 0.7350\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5483 - accuracy: 0.7159 - val_loss: 0.7039 - val_accuracy: 0.6900\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5286 - accuracy: 0.7408 - val_loss: 0.5980 - val_accuracy: 0.7250\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.5242 - accuracy: 0.7364 - val_loss: 0.7336 - val_accuracy: 0.7150\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4914 - accuracy: 0.7494 - val_loss: 0.5779 - val_accuracy: 0.6700\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5215 - accuracy: 0.7147 - val_loss: 0.5599 - val_accuracy: 0.7650\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4518 - accuracy: 0.7919 - val_loss: 0.6150 - val_accuracy: 0.6800\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.5283 - accuracy: 0.7429 - val_loss: 0.5104 - val_accuracy: 0.7350\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4710 - accuracy: 0.7878 - val_loss: 0.5012 - val_accuracy: 0.8100\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4676 - accuracy: 0.7920 - val_loss: 0.5171 - val_accuracy: 0.7550\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4786 - accuracy: 0.7720 - val_loss: 0.4934 - val_accuracy: 0.7750\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4634 - accuracy: 0.7950 - val_loss: 0.5241 - val_accuracy: 0.7800\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4581 - accuracy: 0.7915 - val_loss: 0.4445 - val_accuracy: 0.7900\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3961 - accuracy: 0.8267 - val_loss: 0.5313 - val_accuracy: 0.7250\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3963 - accuracy: 0.8172 - val_loss: 0.7311 - val_accuracy: 0.8000\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3838 - accuracy: 0.8403 - val_loss: 0.5227 - val_accuracy: 0.7350\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3912 - accuracy: 0.8215 - val_loss: 0.5442 - val_accuracy: 0.7800\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4363 - accuracy: 0.7998 - val_loss: 0.4274 - val_accuracy: 0.8000\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3877 - accuracy: 0.8440 - val_loss: 0.4902 - val_accuracy: 0.7900\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3683 - accuracy: 0.8218 - val_loss: 0.5145 - val_accuracy: 0.7900\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.4061 - accuracy: 0.8064 - val_loss: 0.4668 - val_accuracy: 0.7850\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3733 - accuracy: 0.8240 - val_loss: 0.4545 - val_accuracy: 0.7600\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3220 - accuracy: 0.8601 - val_loss: 0.5768 - val_accuracy: 0.7550\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3680 - accuracy: 0.8431 - val_loss: 0.4536 - val_accuracy: 0.8150\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3652 - accuracy: 0.8281 - val_loss: 0.4687 - val_accuracy: 0.7700\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3163 - accuracy: 0.8663 - val_loss: 0.4323 - val_accuracy: 0.7900\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3545 - accuracy: 0.8513 - val_loss: 0.5141 - val_accuracy: 0.8000\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3089 - accuracy: 0.8787 - val_loss: 0.5356 - val_accuracy: 0.7550\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2709 - accuracy: 0.8981 - val_loss: 0.3977 - val_accuracy: 0.8450\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3099 - accuracy: 0.8818 - val_loss: 0.5181 - val_accuracy: 0.7500\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3095 - accuracy: 0.8471 - val_loss: 0.4712 - val_accuracy: 0.7750\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3059 - accuracy: 0.8663 - val_loss: 0.4218 - val_accuracy: 0.8150\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.3144 - accuracy: 0.8686 - val_loss: 0.4016 - val_accuracy: 0.8200\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2910 - accuracy: 0.8782 - val_loss: 0.3845 - val_accuracy: 0.8400\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2463 - accuracy: 0.8971 - val_loss: 0.5009 - val_accuracy: 0.8000\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3258 - accuracy: 0.8644 - val_loss: 0.4094 - val_accuracy: 0.8100\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2565 - accuracy: 0.8948 - val_loss: 0.4350 - val_accuracy: 0.8100\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2048 - accuracy: 0.9235 - val_loss: 0.4928 - val_accuracy: 0.8000\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2557 - accuracy: 0.9009 - val_loss: 0.4295 - val_accuracy: 0.8250\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2871 - accuracy: 0.8674 - val_loss: 0.3898 - val_accuracy: 0.8550\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2454 - accuracy: 0.8947 - val_loss: 0.3814 - val_accuracy: 0.8250\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2376 - accuracy: 0.8999 - val_loss: 0.4878 - val_accuracy: 0.8050\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2772 - accuracy: 0.8793 - val_loss: 0.3898 - val_accuracy: 0.8250\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.2604 - accuracy: 0.8848 - val_loss: 0.4098 - val_accuracy: 0.8300\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2247 - accuracy: 0.8980 - val_loss: 0.4531 - val_accuracy: 0.7900\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 3s 14ms/step - loss: 0.2096 - accuracy: 0.9179 - val_loss: 0.3599 - val_accuracy: 0.8400\n",
            "Score for fold 4: loss of 0.35994094610214233; accuracy of 83.99999737739563%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 6, 1, 32)     320         input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 6, 1, 64)     18496       conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 6, 1, 64)     256         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_108 (Add)                   (None, 6, 1, 64)     0           conv2d_154[0][0]                 \n",
            "                                                                 conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 6, 1, 64)     256         add_108[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_109 (Add)                   (None, 6, 1, 64)     0           conv2d_154[0][0]                 \n",
            "                                                                 conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 6, 1, 64)     256         add_109[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_110 (Add)                   (None, 6, 1, 64)     0           conv2d_154[0][0]                 \n",
            "                                                                 conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 6, 1, 64)     256         add_110[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 6, 1, 64)     36928       dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 6, 1, 64)     256         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_111 (Add)                   (None, 6, 1, 64)     0           conv2d_158[0][0]                 \n",
            "                                                                 conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 6, 1, 64)     256         add_111[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_112 (Add)                   (None, 6, 1, 64)     0           conv2d_158[0][0]                 \n",
            "                                                                 conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 6, 1, 64)     256         add_112[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_113 (Add)                   (None, 6, 1, 64)     0           conv2d_158[0][0]                 \n",
            "                                                                 conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 6, 1, 64)     256         add_113[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 3, 1, 64)     36928       dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 3, 1, 64)     256         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_114 (Add)                   (None, 3, 1, 64)     0           conv2d_162[0][0]                 \n",
            "                                                                 conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 3, 1, 64)     256         add_114[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_115 (Add)                   (None, 3, 1, 64)     0           conv2d_162[0][0]                 \n",
            "                                                                 conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 3, 1, 64)     256         add_115[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_116 (Add)                   (None, 3, 1, 64)     0           conv2d_162[0][0]                 \n",
            "                                                                 conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 3, 1, 64)     256         add_116[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 3, 1, 64)     36928       dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 3, 1, 64)     256         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_117 (Add)                   (None, 3, 1, 64)     0           conv2d_166[0][0]                 \n",
            "                                                                 conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 3, 1, 64)     256         add_117[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_118 (Add)                   (None, 3, 1, 64)     0           conv2d_166[0][0]                 \n",
            "                                                                 conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 3, 1, 64)     256         add_118[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_119 (Add)                   (None, 3, 1, 64)     0           conv2d_166[0][0]                 \n",
            "                                                                 conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 3, 1, 64)     256         add_119[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (None, 128)          0           max_pooling2d_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 2)            258         flatten_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 5s 17ms/step - loss: 0.8829 - accuracy: 0.5709 - val_loss: 0.6773 - val_accuracy: 0.6100\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.7287 - accuracy: 0.6006 - val_loss: 0.5832 - val_accuracy: 0.7350\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 3s 15ms/step - loss: 0.6857 - accuracy: 0.6606 - val_loss: 0.7449 - val_accuracy: 0.6700\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.6228 - accuracy: 0.6848 - val_loss: 0.6583 - val_accuracy: 0.6850\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.5357 - accuracy: 0.7416 - val_loss: 0.5693 - val_accuracy: 0.7300\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.5771 - accuracy: 0.7243 - val_loss: 0.5501 - val_accuracy: 0.7300\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4817 - accuracy: 0.7701 - val_loss: 0.4936 - val_accuracy: 0.7500\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4993 - accuracy: 0.7793 - val_loss: 0.4723 - val_accuracy: 0.7600\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4852 - accuracy: 0.7732 - val_loss: 0.4518 - val_accuracy: 0.7800\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.4456 - accuracy: 0.8001 - val_loss: 0.4690 - val_accuracy: 0.8050\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4482 - accuracy: 0.8030 - val_loss: 0.4737 - val_accuracy: 0.7800\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4629 - accuracy: 0.7930 - val_loss: 0.5119 - val_accuracy: 0.7450\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4504 - accuracy: 0.7983 - val_loss: 0.5317 - val_accuracy: 0.7400\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4404 - accuracy: 0.8087 - val_loss: 0.4192 - val_accuracy: 0.8600\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4190 - accuracy: 0.8305 - val_loss: 0.4789 - val_accuracy: 0.7950\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.4468 - accuracy: 0.8107 - val_loss: 0.4243 - val_accuracy: 0.8100\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4478 - accuracy: 0.8023 - val_loss: 0.3974 - val_accuracy: 0.8250\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3919 - accuracy: 0.8295 - val_loss: 0.3451 - val_accuracy: 0.8400\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.4459 - accuracy: 0.8171 - val_loss: 0.4383 - val_accuracy: 0.7900\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.4007 - accuracy: 0.8248 - val_loss: 0.4012 - val_accuracy: 0.8150\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3863 - accuracy: 0.8544 - val_loss: 0.3913 - val_accuracy: 0.7900\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3544 - accuracy: 0.8200 - val_loss: 0.3760 - val_accuracy: 0.8400\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3705 - accuracy: 0.8512 - val_loss: 0.3598 - val_accuracy: 0.8400\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3332 - accuracy: 0.8499 - val_loss: 0.3935 - val_accuracy: 0.8450\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3625 - accuracy: 0.8443 - val_loss: 0.4131 - val_accuracy: 0.8100\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3527 - accuracy: 0.8603 - val_loss: 0.4151 - val_accuracy: 0.8350\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3511 - accuracy: 0.8625 - val_loss: 0.4038 - val_accuracy: 0.8450\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3586 - accuracy: 0.8489 - val_loss: 0.3542 - val_accuracy: 0.8600\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3780 - accuracy: 0.8374 - val_loss: 0.3633 - val_accuracy: 0.8500\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2916 - accuracy: 0.8713 - val_loss: 0.3966 - val_accuracy: 0.8350\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3123 - accuracy: 0.8765 - val_loss: 0.3516 - val_accuracy: 0.8450\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3148 - accuracy: 0.8891 - val_loss: 0.2674 - val_accuracy: 0.8750\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3171 - accuracy: 0.8430 - val_loss: 0.3614 - val_accuracy: 0.8350\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2660 - accuracy: 0.8926 - val_loss: 0.3118 - val_accuracy: 0.8550\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2699 - accuracy: 0.8995 - val_loss: 0.3558 - val_accuracy: 0.8500\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2456 - accuracy: 0.9135 - val_loss: 0.3265 - val_accuracy: 0.8250\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2978 - accuracy: 0.8724 - val_loss: 0.3478 - val_accuracy: 0.8750\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2446 - accuracy: 0.9120 - val_loss: 0.3331 - val_accuracy: 0.8550\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.3573 - accuracy: 0.8640 - val_loss: 0.2894 - val_accuracy: 0.8850\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2874 - accuracy: 0.8825 - val_loss: 0.2985 - val_accuracy: 0.8950\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2249 - accuracy: 0.9160 - val_loss: 0.3000 - val_accuracy: 0.8700\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2616 - accuracy: 0.8941 - val_loss: 0.4096 - val_accuracy: 0.8450\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2425 - accuracy: 0.9028 - val_loss: 0.3232 - val_accuracy: 0.8650\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2052 - accuracy: 0.9200 - val_loss: 0.2676 - val_accuracy: 0.8850\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2595 - accuracy: 0.8996 - val_loss: 0.2433 - val_accuracy: 0.8950\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.1904 - accuracy: 0.9283 - val_loss: 0.2910 - val_accuracy: 0.8600\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.1649 - accuracy: 0.9330 - val_loss: 0.3043 - val_accuracy: 0.8850\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2351 - accuracy: 0.9028 - val_loss: 0.3154 - val_accuracy: 0.8450\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2116 - accuracy: 0.9170 - val_loss: 0.2840 - val_accuracy: 0.8550\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.2288 - accuracy: 0.9175 - val_loss: 0.2781 - val_accuracy: 0.8800\n",
            "Score for fold 5: loss of 0.2780719995498657; accuracy of 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.3615168035030365 - Accuracy: 86.56716346740723%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.4244372248649597 - Accuracy: 82.08954930305481%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.34077614545822144 - Accuracy: 81.99999928474426%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.35994094610214233 - Accuracy: 83.99999737739563%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.2780719995498657 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 84.53134179115295 (+- 2.401188400982056)\n",
            "> Loss: 0.35294862389564513\n",
            "------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 6, 1, 32)     320         input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 6, 1, 64)     18496       conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 6, 1, 64)     256         conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_120 (Add)                   (None, 6, 1, 64)     0           conv2d_171[0][0]                 \n",
            "                                                                 conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 6, 1, 64)     256         add_120[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_121 (Add)                   (None, 6, 1, 64)     0           conv2d_171[0][0]                 \n",
            "                                                                 conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 6, 1, 64)     256         add_121[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_122 (Add)                   (None, 6, 1, 64)     0           conv2d_171[0][0]                 \n",
            "                                                                 conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 6, 1, 64)     256         add_122[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 6, 1, 64)     36928       dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 6, 1, 64)     256         conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_123 (Add)                   (None, 6, 1, 64)     0           conv2d_175[0][0]                 \n",
            "                                                                 conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 6, 1, 64)     256         add_123[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_124 (Add)                   (None, 6, 1, 64)     0           conv2d_175[0][0]                 \n",
            "                                                                 conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 6, 1, 64)     256         add_124[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_125 (Add)                   (None, 6, 1, 64)     0           conv2d_175[0][0]                 \n",
            "                                                                 conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 6, 1, 64)     256         add_125[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 3, 1, 64)     36928       dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 3, 1, 64)     256         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_126 (Add)                   (None, 3, 1, 64)     0           conv2d_179[0][0]                 \n",
            "                                                                 conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 3, 1, 64)     256         add_126[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_127 (Add)                   (None, 3, 1, 64)     0           conv2d_179[0][0]                 \n",
            "                                                                 conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 3, 1, 64)     256         add_127[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_128 (Add)                   (None, 3, 1, 64)     0           conv2d_179[0][0]                 \n",
            "                                                                 conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 3, 1, 64)     256         add_128[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 3, 1, 64)     36928       dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 3, 1, 64)     256         conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_129 (Add)                   (None, 3, 1, 64)     0           conv2d_183[0][0]                 \n",
            "                                                                 conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 3, 1, 64)     256         add_129[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_130 (Add)                   (None, 3, 1, 64)     0           conv2d_183[0][0]                 \n",
            "                                                                 conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 3, 1, 64)     256         add_130[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_131 (Add)                   (None, 3, 1, 64)     0           conv2d_183[0][0]                 \n",
            "                                                                 conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 3, 1, 64)     256         add_131[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 128)          0           max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 2)            258         flatten_10[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 6s 20ms/step - loss: 1.0334 - accuracy: 0.5329 - val_loss: 0.6052 - val_accuracy: 0.6070\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.7600 - accuracy: 0.6041 - val_loss: 0.5643 - val_accuracy: 0.7214\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.5949 - accuracy: 0.7141 - val_loss: 0.6646 - val_accuracy: 0.6567\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.5829 - accuracy: 0.6955 - val_loss: 0.4384 - val_accuracy: 0.7910\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.5485 - accuracy: 0.7458 - val_loss: 0.4444 - val_accuracy: 0.8010\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4826 - accuracy: 0.7709 - val_loss: 0.4130 - val_accuracy: 0.8109\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4981 - accuracy: 0.7619 - val_loss: 0.4286 - val_accuracy: 0.8458\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4790 - accuracy: 0.7949 - val_loss: 0.4235 - val_accuracy: 0.8109\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.4387 - accuracy: 0.8143 - val_loss: 0.4307 - val_accuracy: 0.8209\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3877 - accuracy: 0.8316 - val_loss: 0.4854 - val_accuracy: 0.8159\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4016 - accuracy: 0.8362 - val_loss: 0.3847 - val_accuracy: 0.8358\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3886 - accuracy: 0.8269 - val_loss: 0.3951 - val_accuracy: 0.8308\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3595 - accuracy: 0.8633 - val_loss: 0.3577 - val_accuracy: 0.8408\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3623 - accuracy: 0.8401 - val_loss: 0.4153 - val_accuracy: 0.8159\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3878 - accuracy: 0.8408 - val_loss: 0.3233 - val_accuracy: 0.8507\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3604 - accuracy: 0.8676 - val_loss: 0.3671 - val_accuracy: 0.7960\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3277 - accuracy: 0.8643 - val_loss: 0.3709 - val_accuracy: 0.8607\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3379 - accuracy: 0.8510 - val_loss: 0.4036 - val_accuracy: 0.8060\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3468 - accuracy: 0.8547 - val_loss: 0.2741 - val_accuracy: 0.8955\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3426 - accuracy: 0.8622 - val_loss: 0.3255 - val_accuracy: 0.8657\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3179 - accuracy: 0.8872 - val_loss: 0.3253 - val_accuracy: 0.8706\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.3201 - accuracy: 0.8710 - val_loss: 0.3632 - val_accuracy: 0.8358\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3153 - accuracy: 0.8787 - val_loss: 0.3418 - val_accuracy: 0.8408\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3207 - accuracy: 0.8792 - val_loss: 0.2892 - val_accuracy: 0.8856\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2296 - accuracy: 0.9134 - val_loss: 0.4010 - val_accuracy: 0.8607\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3189 - accuracy: 0.8695 - val_loss: 0.2947 - val_accuracy: 0.8955\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2965 - accuracy: 0.8902 - val_loss: 0.3451 - val_accuracy: 0.8557\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2463 - accuracy: 0.9014 - val_loss: 0.2987 - val_accuracy: 0.9005\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2494 - accuracy: 0.9064 - val_loss: 0.2708 - val_accuracy: 0.8856\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2347 - accuracy: 0.8936 - val_loss: 0.3202 - val_accuracy: 0.8657\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2152 - accuracy: 0.9177 - val_loss: 0.3038 - val_accuracy: 0.8657\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2379 - accuracy: 0.9067 - val_loss: 0.2913 - val_accuracy: 0.8856\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2346 - accuracy: 0.9026 - val_loss: 0.3217 - val_accuracy: 0.8706\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2084 - accuracy: 0.9112 - val_loss: 0.2874 - val_accuracy: 0.9055\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1885 - accuracy: 0.9277 - val_loss: 0.2477 - val_accuracy: 0.9104\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2162 - accuracy: 0.8961 - val_loss: 0.2928 - val_accuracy: 0.8955\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2056 - accuracy: 0.9115 - val_loss: 0.2689 - val_accuracy: 0.8856\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1801 - accuracy: 0.9246 - val_loss: 0.3281 - val_accuracy: 0.8657\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2615 - accuracy: 0.8823 - val_loss: 0.3078 - val_accuracy: 0.8955\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2087 - accuracy: 0.9196 - val_loss: 0.2639 - val_accuracy: 0.9055\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2346 - accuracy: 0.8966 - val_loss: 0.2846 - val_accuracy: 0.8806\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2028 - accuracy: 0.9044 - val_loss: 0.2889 - val_accuracy: 0.8856\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1664 - accuracy: 0.9282 - val_loss: 0.2554 - val_accuracy: 0.9005\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1592 - accuracy: 0.9321 - val_loss: 0.2429 - val_accuracy: 0.9204\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1321 - accuracy: 0.9551 - val_loss: 0.2765 - val_accuracy: 0.9055\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3512 - accuracy: 0.8985 - val_loss: 0.3120 - val_accuracy: 0.8905\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2330 - accuracy: 0.9010 - val_loss: 0.2720 - val_accuracy: 0.9055\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2707 - accuracy: 0.9031 - val_loss: 0.2478 - val_accuracy: 0.9204\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1239 - accuracy: 0.9560 - val_loss: 0.3357 - val_accuracy: 0.8557\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1621 - accuracy: 0.9237 - val_loss: 0.2763 - val_accuracy: 0.9055\n",
            "Score for fold 1: loss of 0.27629175782203674; accuracy of 90.54726362228394%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 6, 1, 32)     320         input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 6, 1, 64)     18496       conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 6, 1, 64)     256         conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_132 (Add)                   (None, 6, 1, 64)     0           conv2d_188[0][0]                 \n",
            "                                                                 conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 6, 1, 64)     256         add_132[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_133 (Add)                   (None, 6, 1, 64)     0           conv2d_188[0][0]                 \n",
            "                                                                 conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 6, 1, 64)     256         add_133[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_134 (Add)                   (None, 6, 1, 64)     0           conv2d_188[0][0]                 \n",
            "                                                                 conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 6, 1, 64)     256         add_134[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 6, 1, 64)     36928       dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 6, 1, 64)     256         conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_135 (Add)                   (None, 6, 1, 64)     0           conv2d_192[0][0]                 \n",
            "                                                                 conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 6, 1, 64)     256         add_135[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_136 (Add)                   (None, 6, 1, 64)     0           conv2d_192[0][0]                 \n",
            "                                                                 conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 6, 1, 64)     256         add_136[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_137 (Add)                   (None, 6, 1, 64)     0           conv2d_192[0][0]                 \n",
            "                                                                 conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 6, 1, 64)     256         add_137[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_22[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 3, 1, 64)     36928       dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 3, 1, 64)     256         conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_138 (Add)                   (None, 3, 1, 64)     0           conv2d_196[0][0]                 \n",
            "                                                                 conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 3, 1, 64)     256         add_138[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_139 (Add)                   (None, 3, 1, 64)     0           conv2d_196[0][0]                 \n",
            "                                                                 conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 3, 1, 64)     256         add_139[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_140 (Add)                   (None, 3, 1, 64)     0           conv2d_196[0][0]                 \n",
            "                                                                 conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 3, 1, 64)     256         add_140[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 3, 1, 64)     36928       dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 3, 1, 64)     256         conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_141 (Add)                   (None, 3, 1, 64)     0           conv2d_200[0][0]                 \n",
            "                                                                 conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 3, 1, 64)     256         add_141[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_142 (Add)                   (None, 3, 1, 64)     0           conv2d_200[0][0]                 \n",
            "                                                                 conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 3, 1, 64)     256         add_142[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_143 (Add)                   (None, 3, 1, 64)     0           conv2d_200[0][0]                 \n",
            "                                                                 conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 3, 1, 64)     256         add_143[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 128)          0           max_pooling2d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 2)            258         flatten_11[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 6s 20ms/step - loss: 0.8662 - accuracy: 0.5722 - val_loss: 0.8723 - val_accuracy: 0.6169\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.6640 - accuracy: 0.6356 - val_loss: 0.5505 - val_accuracy: 0.6766\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.6741 - accuracy: 0.6042 - val_loss: 0.5605 - val_accuracy: 0.7264\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.5962 - accuracy: 0.6805 - val_loss: 0.4849 - val_accuracy: 0.7562\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.5727 - accuracy: 0.7229 - val_loss: 0.6147 - val_accuracy: 0.6716\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.4976 - accuracy: 0.7552 - val_loss: 0.5864 - val_accuracy: 0.7363\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.4914 - accuracy: 0.7920 - val_loss: 0.7160 - val_accuracy: 0.7463\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.4538 - accuracy: 0.8119 - val_loss: 0.5483 - val_accuracy: 0.7861\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.4114 - accuracy: 0.8184 - val_loss: 0.5640 - val_accuracy: 0.7562\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3983 - accuracy: 0.8383 - val_loss: 0.6186 - val_accuracy: 0.7413\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3767 - accuracy: 0.8423 - val_loss: 0.4345 - val_accuracy: 0.8010\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3916 - accuracy: 0.8201 - val_loss: 0.4517 - val_accuracy: 0.7662\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3263 - accuracy: 0.8543 - val_loss: 0.4228 - val_accuracy: 0.8060\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3183 - accuracy: 0.8579 - val_loss: 0.4953 - val_accuracy: 0.7711\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3855 - accuracy: 0.8443 - val_loss: 0.5831 - val_accuracy: 0.6766\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3613 - accuracy: 0.8303 - val_loss: 0.4102 - val_accuracy: 0.7861\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3279 - accuracy: 0.8492 - val_loss: 0.3810 - val_accuracy: 0.8259\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3311 - accuracy: 0.8605 - val_loss: 0.3872 - val_accuracy: 0.8358\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2808 - accuracy: 0.8796 - val_loss: 0.3783 - val_accuracy: 0.8259\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2708 - accuracy: 0.8973 - val_loss: 0.3950 - val_accuracy: 0.8308\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2884 - accuracy: 0.8784 - val_loss: 0.3904 - val_accuracy: 0.8109\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.3205 - accuracy: 0.8557 - val_loss: 0.4390 - val_accuracy: 0.8060\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2600 - accuracy: 0.8929 - val_loss: 0.3638 - val_accuracy: 0.8657\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2628 - accuracy: 0.8751 - val_loss: 0.3699 - val_accuracy: 0.8159\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2480 - accuracy: 0.9053 - val_loss: 0.3964 - val_accuracy: 0.8109\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2527 - accuracy: 0.9043 - val_loss: 0.3885 - val_accuracy: 0.8408\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2661 - accuracy: 0.8838 - val_loss: 0.4719 - val_accuracy: 0.7811\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2414 - accuracy: 0.8995 - val_loss: 0.4160 - val_accuracy: 0.8010\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3142 - accuracy: 0.8683 - val_loss: 0.3457 - val_accuracy: 0.8557\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2334 - accuracy: 0.9187 - val_loss: 0.4312 - val_accuracy: 0.8109\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2957 - accuracy: 0.8699 - val_loss: 0.3711 - val_accuracy: 0.8358\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1921 - accuracy: 0.9141 - val_loss: 0.3127 - val_accuracy: 0.8607\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2078 - accuracy: 0.9165 - val_loss: 0.3105 - val_accuracy: 0.8557\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1706 - accuracy: 0.9379 - val_loss: 0.3636 - val_accuracy: 0.8557\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2821 - accuracy: 0.9017 - val_loss: 0.3382 - val_accuracy: 0.8657\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1423 - accuracy: 0.9446 - val_loss: 0.3418 - val_accuracy: 0.8507\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2566 - accuracy: 0.8896 - val_loss: 0.4031 - val_accuracy: 0.7960\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1960 - accuracy: 0.9278 - val_loss: 0.3540 - val_accuracy: 0.8308\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1865 - accuracy: 0.9261 - val_loss: 0.3820 - val_accuracy: 0.8060\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2150 - accuracy: 0.9123 - val_loss: 0.3523 - val_accuracy: 0.8408\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1883 - accuracy: 0.9337 - val_loss: 0.4340 - val_accuracy: 0.8109\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2013 - accuracy: 0.9302 - val_loss: 0.3500 - val_accuracy: 0.8408\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1760 - accuracy: 0.9264 - val_loss: 0.3407 - val_accuracy: 0.8507\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1764 - accuracy: 0.9327 - val_loss: 0.3973 - val_accuracy: 0.8358\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.1420 - accuracy: 0.9456 - val_loss: 0.3244 - val_accuracy: 0.8408\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.1940 - accuracy: 0.9198 - val_loss: 0.3051 - val_accuracy: 0.8607\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2297 - accuracy: 0.9054 - val_loss: 0.3297 - val_accuracy: 0.8507\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1876 - accuracy: 0.9146 - val_loss: 0.3070 - val_accuracy: 0.8557\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1547 - accuracy: 0.9312 - val_loss: 0.3351 - val_accuracy: 0.8458\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1974 - accuracy: 0.9277 - val_loss: 0.3692 - val_accuracy: 0.8458\n",
            "Score for fold 2: loss of 0.3691904544830322; accuracy of 84.57711338996887%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 6, 1, 32)     320         input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 6, 1, 64)     18496       conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 6, 1, 64)     256         conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_144 (Add)                   (None, 6, 1, 64)     0           conv2d_205[0][0]                 \n",
            "                                                                 conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 6, 1, 64)     256         add_144[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_145 (Add)                   (None, 6, 1, 64)     0           conv2d_205[0][0]                 \n",
            "                                                                 conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 6, 1, 64)     256         add_145[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_146 (Add)                   (None, 6, 1, 64)     0           conv2d_205[0][0]                 \n",
            "                                                                 conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 6, 1, 64)     256         add_146[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 6, 1, 64)     36928       dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 6, 1, 64)     256         conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_147 (Add)                   (None, 6, 1, 64)     0           conv2d_209[0][0]                 \n",
            "                                                                 conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 6, 1, 64)     256         add_147[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_148 (Add)                   (None, 6, 1, 64)     0           conv2d_209[0][0]                 \n",
            "                                                                 conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 6, 1, 64)     256         add_148[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_149 (Add)                   (None, 6, 1, 64)     0           conv2d_209[0][0]                 \n",
            "                                                                 conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 6, 1, 64)     256         add_149[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_24[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 3, 1, 64)     36928       dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 3, 1, 64)     256         conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_150 (Add)                   (None, 3, 1, 64)     0           conv2d_213[0][0]                 \n",
            "                                                                 conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 3, 1, 64)     256         add_150[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_151 (Add)                   (None, 3, 1, 64)     0           conv2d_213[0][0]                 \n",
            "                                                                 conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 3, 1, 64)     256         add_151[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_152 (Add)                   (None, 3, 1, 64)     0           conv2d_213[0][0]                 \n",
            "                                                                 conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 3, 1, 64)     256         add_152[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 3, 1, 64)     36928       dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 3, 1, 64)     256         conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_153 (Add)                   (None, 3, 1, 64)     0           conv2d_217[0][0]                 \n",
            "                                                                 conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 3, 1, 64)     256         add_153[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_154 (Add)                   (None, 3, 1, 64)     0           conv2d_217[0][0]                 \n",
            "                                                                 conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 3, 1, 64)     256         add_154[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_155 (Add)                   (None, 3, 1, 64)     0           conv2d_217[0][0]                 \n",
            "                                                                 conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 3, 1, 64)     256         add_155[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 128)          0           max_pooling2d_25[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 2)            258         flatten_12[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 6s 22ms/step - loss: 1.0466 - accuracy: 0.6069 - val_loss: 0.6606 - val_accuracy: 0.5800\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 3s 16ms/step - loss: 0.7390 - accuracy: 0.6461 - val_loss: 1.3912 - val_accuracy: 0.6050\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.6277 - accuracy: 0.6809 - val_loss: 0.5747 - val_accuracy: 0.7050\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.5060 - accuracy: 0.7868 - val_loss: 0.6663 - val_accuracy: 0.7100\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.5165 - accuracy: 0.7699 - val_loss: 0.7307 - val_accuracy: 0.7300\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4729 - accuracy: 0.7991 - val_loss: 0.5297 - val_accuracy: 0.7350\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.3940 - accuracy: 0.8447 - val_loss: 1.0968 - val_accuracy: 0.6700\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4520 - accuracy: 0.8064 - val_loss: 0.4991 - val_accuracy: 0.7900\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.4096 - accuracy: 0.8241 - val_loss: 0.6119 - val_accuracy: 0.7550\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3630 - accuracy: 0.8498 - val_loss: 0.5284 - val_accuracy: 0.7650\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3728 - accuracy: 0.8260 - val_loss: 0.5335 - val_accuracy: 0.7650\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3288 - accuracy: 0.8480 - val_loss: 0.7130 - val_accuracy: 0.7800\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.3596 - accuracy: 0.8398 - val_loss: 0.4872 - val_accuracy: 0.7650\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.3417 - accuracy: 0.8668 - val_loss: 0.5591 - val_accuracy: 0.8000\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.3321 - accuracy: 0.8711 - val_loss: 0.7058 - val_accuracy: 0.7550\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2969 - accuracy: 0.8783 - val_loss: 0.5190 - val_accuracy: 0.7950\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.3274 - accuracy: 0.8657 - val_loss: 0.4825 - val_accuracy: 0.7950\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.3637 - accuracy: 0.8590 - val_loss: 0.6189 - val_accuracy: 0.7900\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2795 - accuracy: 0.8777 - val_loss: 0.4929 - val_accuracy: 0.7700\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2864 - accuracy: 0.8930 - val_loss: 0.6230 - val_accuracy: 0.7950\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2937 - accuracy: 0.8581 - val_loss: 0.6127 - val_accuracy: 0.7850\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2348 - accuracy: 0.9118 - val_loss: 0.5512 - val_accuracy: 0.7650\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3087 - accuracy: 0.8722 - val_loss: 0.5031 - val_accuracy: 0.7900\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2990 - accuracy: 0.8739 - val_loss: 0.4563 - val_accuracy: 0.8100\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.3042 - accuracy: 0.8797 - val_loss: 0.5174 - val_accuracy: 0.7900\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2608 - accuracy: 0.8963 - val_loss: 0.5147 - val_accuracy: 0.7950\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2330 - accuracy: 0.9012 - val_loss: 0.3969 - val_accuracy: 0.8250\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2104 - accuracy: 0.9125 - val_loss: 0.5209 - val_accuracy: 0.7550\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2500 - accuracy: 0.8999 - val_loss: 0.5266 - val_accuracy: 0.8050\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2341 - accuracy: 0.9147 - val_loss: 0.4836 - val_accuracy: 0.7850\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.1707 - accuracy: 0.9515 - val_loss: 0.4035 - val_accuracy: 0.8400\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2055 - accuracy: 0.9164 - val_loss: 0.4257 - val_accuracy: 0.8300\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2351 - accuracy: 0.9154 - val_loss: 0.3740 - val_accuracy: 0.8250\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.1986 - accuracy: 0.9370 - val_loss: 0.4026 - val_accuracy: 0.7950\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2011 - accuracy: 0.9132 - val_loss: 0.6229 - val_accuracy: 0.7500\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2578 - accuracy: 0.8978 - val_loss: 0.4348 - val_accuracy: 0.8250\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1440 - accuracy: 0.9542 - val_loss: 0.3809 - val_accuracy: 0.8000\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2031 - accuracy: 0.9183 - val_loss: 0.5059 - val_accuracy: 0.8150\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2126 - accuracy: 0.9164 - val_loss: 0.5094 - val_accuracy: 0.7900\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.1826 - accuracy: 0.9265 - val_loss: 0.4587 - val_accuracy: 0.8200\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.1999 - accuracy: 0.9220 - val_loss: 0.3898 - val_accuracy: 0.8550\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1544 - accuracy: 0.9496 - val_loss: 0.4280 - val_accuracy: 0.8300\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.1627 - accuracy: 0.9426 - val_loss: 0.3878 - val_accuracy: 0.8300\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.1511 - accuracy: 0.9442 - val_loss: 0.5435 - val_accuracy: 0.8050\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.1403 - accuracy: 0.9459 - val_loss: 0.3990 - val_accuracy: 0.8350\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.1522 - accuracy: 0.9500 - val_loss: 0.5243 - val_accuracy: 0.8150\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.1715 - accuracy: 0.9451 - val_loss: 0.3599 - val_accuracy: 0.8450\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.1241 - accuracy: 0.9565 - val_loss: 0.3149 - val_accuracy: 0.8600\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.1265 - accuracy: 0.9600 - val_loss: 0.3764 - val_accuracy: 0.8200\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1503 - accuracy: 0.9486 - val_loss: 0.4147 - val_accuracy: 0.8250\n",
            "Score for fold 3: loss of 0.4147241711616516; accuracy of 82.4999988079071%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 6, 1, 32)     320         input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 6, 1, 64)     18496       conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 6, 1, 64)     256         conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_156 (Add)                   (None, 6, 1, 64)     0           conv2d_222[0][0]                 \n",
            "                                                                 conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 6, 1, 64)     256         add_156[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_157 (Add)                   (None, 6, 1, 64)     0           conv2d_222[0][0]                 \n",
            "                                                                 conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 6, 1, 64)     256         add_157[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_158 (Add)                   (None, 6, 1, 64)     0           conv2d_222[0][0]                 \n",
            "                                                                 conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 6, 1, 64)     256         add_158[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 6, 1, 64)     36928       dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 6, 1, 64)     256         conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_159 (Add)                   (None, 6, 1, 64)     0           conv2d_226[0][0]                 \n",
            "                                                                 conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 6, 1, 64)     256         add_159[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_160 (Add)                   (None, 6, 1, 64)     0           conv2d_226[0][0]                 \n",
            "                                                                 conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 6, 1, 64)     256         add_160[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_161 (Add)                   (None, 6, 1, 64)     0           conv2d_226[0][0]                 \n",
            "                                                                 conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 6, 1, 64)     256         add_161[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_26[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 3, 1, 64)     36928       dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 3, 1, 64)     256         conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_162 (Add)                   (None, 3, 1, 64)     0           conv2d_230[0][0]                 \n",
            "                                                                 conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 3, 1, 64)     256         add_162[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_163 (Add)                   (None, 3, 1, 64)     0           conv2d_230[0][0]                 \n",
            "                                                                 conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 3, 1, 64)     256         add_163[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_164 (Add)                   (None, 3, 1, 64)     0           conv2d_230[0][0]                 \n",
            "                                                                 conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 3, 1, 64)     256         add_164[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 3, 1, 64)     36928       dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 3, 1, 64)     256         conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_165 (Add)                   (None, 3, 1, 64)     0           conv2d_234[0][0]                 \n",
            "                                                                 conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 3, 1, 64)     256         add_165[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_166 (Add)                   (None, 3, 1, 64)     0           conv2d_234[0][0]                 \n",
            "                                                                 conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 3, 1, 64)     256         add_166[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_167 (Add)                   (None, 3, 1, 64)     0           conv2d_234[0][0]                 \n",
            "                                                                 conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 3, 1, 64)     256         add_167[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_13 (Flatten)            (None, 128)          0           max_pooling2d_27[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 2)            258         flatten_13[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 6s 19ms/step - loss: 0.9519 - accuracy: 0.5639 - val_loss: 1.1767 - val_accuracy: 0.5400\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.6430 - accuracy: 0.6488 - val_loss: 0.6184 - val_accuracy: 0.6400\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.6094 - accuracy: 0.6899 - val_loss: 0.6528 - val_accuracy: 0.7100\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.5697 - accuracy: 0.7158 - val_loss: 0.4483 - val_accuracy: 0.7750\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.5290 - accuracy: 0.7634 - val_loss: 0.4313 - val_accuracy: 0.7900\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.5149 - accuracy: 0.7485 - val_loss: 0.4441 - val_accuracy: 0.8250\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.4376 - accuracy: 0.7908 - val_loss: 0.4519 - val_accuracy: 0.8150\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4680 - accuracy: 0.7794 - val_loss: 0.3489 - val_accuracy: 0.8400\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.4290 - accuracy: 0.8182 - val_loss: 0.3501 - val_accuracy: 0.8300\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.4433 - accuracy: 0.7992 - val_loss: 0.4188 - val_accuracy: 0.8200\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4143 - accuracy: 0.8163 - val_loss: 0.3680 - val_accuracy: 0.8400\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3874 - accuracy: 0.8430 - val_loss: 0.4692 - val_accuracy: 0.8100\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.3571 - accuracy: 0.8548 - val_loss: 0.3164 - val_accuracy: 0.8700\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3502 - accuracy: 0.8429 - val_loss: 0.2902 - val_accuracy: 0.8700\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.3250 - accuracy: 0.8796 - val_loss: 0.3514 - val_accuracy: 0.8350\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4095 - accuracy: 0.8204 - val_loss: 0.3359 - val_accuracy: 0.8500\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3275 - accuracy: 0.8737 - val_loss: 0.3211 - val_accuracy: 0.8700\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.3321 - accuracy: 0.8729 - val_loss: 0.3145 - val_accuracy: 0.8750\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2972 - accuracy: 0.8806 - val_loss: 0.3363 - val_accuracy: 0.8700\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.2996 - accuracy: 0.8810 - val_loss: 0.3842 - val_accuracy: 0.8300\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3508 - accuracy: 0.8753 - val_loss: 0.3168 - val_accuracy: 0.8750\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.2646 - accuracy: 0.8866 - val_loss: 0.3772 - val_accuracy: 0.8600\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3463 - accuracy: 0.8731 - val_loss: 0.3524 - val_accuracy: 0.8550\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3004 - accuracy: 0.8798 - val_loss: 0.2790 - val_accuracy: 0.8650\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2755 - accuracy: 0.8876 - val_loss: 0.2467 - val_accuracy: 0.8650\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.2878 - accuracy: 0.8817 - val_loss: 0.2983 - val_accuracy: 0.8800\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.2371 - accuracy: 0.9093 - val_loss: 0.2576 - val_accuracy: 0.8900\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 3s 17ms/step - loss: 0.3120 - accuracy: 0.8705 - val_loss: 0.3117 - val_accuracy: 0.8600\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1993 - accuracy: 0.9256 - val_loss: 0.2882 - val_accuracy: 0.8950\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2301 - accuracy: 0.9083 - val_loss: 0.2244 - val_accuracy: 0.9250\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2205 - accuracy: 0.9131 - val_loss: 0.2949 - val_accuracy: 0.8900\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2672 - accuracy: 0.9147 - val_loss: 0.1990 - val_accuracy: 0.9350\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2031 - accuracy: 0.9200 - val_loss: 0.2155 - val_accuracy: 0.9200\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1822 - accuracy: 0.9366 - val_loss: 0.2634 - val_accuracy: 0.9050\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.1955 - accuracy: 0.9136 - val_loss: 0.2817 - val_accuracy: 0.8950\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2220 - accuracy: 0.9049 - val_loss: 0.2485 - val_accuracy: 0.9000\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 4s 17ms/step - loss: 0.1761 - accuracy: 0.9254 - val_loss: 0.2216 - val_accuracy: 0.9150\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2130 - accuracy: 0.9153 - val_loss: 0.2313 - val_accuracy: 0.9200\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2277 - accuracy: 0.9095 - val_loss: 0.2120 - val_accuracy: 0.9350\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1540 - accuracy: 0.9509 - val_loss: 0.2247 - val_accuracy: 0.9000\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1517 - accuracy: 0.9531 - val_loss: 0.2354 - val_accuracy: 0.9200\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2380 - accuracy: 0.9120 - val_loss: 0.2446 - val_accuracy: 0.9100\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1708 - accuracy: 0.9417 - val_loss: 0.2942 - val_accuracy: 0.8800\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 7s 37ms/step - loss: 0.1705 - accuracy: 0.9358 - val_loss: 0.2564 - val_accuracy: 0.9200\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 6s 29ms/step - loss: 0.1989 - accuracy: 0.9299 - val_loss: 0.2049 - val_accuracy: 0.9250\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1449 - accuracy: 0.9482 - val_loss: 0.2584 - val_accuracy: 0.9050\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1997 - accuracy: 0.9091 - val_loss: 0.2392 - val_accuracy: 0.9150\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2094 - accuracy: 0.9185 - val_loss: 0.2583 - val_accuracy: 0.9150\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.1751 - accuracy: 0.9288 - val_loss: 0.2854 - val_accuracy: 0.8750\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1198 - accuracy: 0.9540 - val_loss: 0.2156 - val_accuracy: 0.9050\n",
            "Score for fold 4: loss of 0.21556930243968964; accuracy of 90.49999713897705%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           [(None, 6, 1, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 6, 1, 32)     320         input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 6, 1, 64)     18496       conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 6, 1, 64)     256         conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_168 (Add)                   (None, 6, 1, 64)     0           conv2d_239[0][0]                 \n",
            "                                                                 conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 6, 1, 64)     256         add_168[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_169 (Add)                   (None, 6, 1, 64)     0           conv2d_239[0][0]                 \n",
            "                                                                 conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 6, 1, 64)     256         add_169[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_170 (Add)                   (None, 6, 1, 64)     0           conv2d_239[0][0]                 \n",
            "                                                                 conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 6, 1, 64)     256         add_170[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 6, 1, 64)     0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 6, 1, 64)     36928       dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 6, 1, 64)     256         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_171 (Add)                   (None, 6, 1, 64)     0           conv2d_243[0][0]                 \n",
            "                                                                 conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 6, 1, 64)     256         add_171[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_172 (Add)                   (None, 6, 1, 64)     0           conv2d_243[0][0]                 \n",
            "                                                                 conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 6, 1, 64)     256         add_172[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 6, 1, 64)     36928       batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_173 (Add)                   (None, 6, 1, 64)     0           conv2d_243[0][0]                 \n",
            "                                                                 conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 6, 1, 64)     256         add_173[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling2D) (None, 3, 1, 64)     0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 3, 1, 64)     0           max_pooling2d_28[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 3, 1, 64)     36928       dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 3, 1, 64)     256         conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_174 (Add)                   (None, 3, 1, 64)     0           conv2d_247[0][0]                 \n",
            "                                                                 conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 3, 1, 64)     256         add_174[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_175 (Add)                   (None, 3, 1, 64)     0           conv2d_247[0][0]                 \n",
            "                                                                 conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 3, 1, 64)     256         add_175[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_176 (Add)                   (None, 3, 1, 64)     0           conv2d_247[0][0]                 \n",
            "                                                                 conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 3, 1, 64)     256         add_176[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 3, 1, 64)     0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 3, 1, 64)     36928       dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 3, 1, 64)     256         conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_177 (Add)                   (None, 3, 1, 64)     0           conv2d_251[0][0]                 \n",
            "                                                                 conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 3, 1, 64)     256         add_177[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_178 (Add)                   (None, 3, 1, 64)     0           conv2d_251[0][0]                 \n",
            "                                                                 conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 3, 1, 64)     256         add_178[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 3, 1, 64)     36928       batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_179 (Add)                   (None, 3, 1, 64)     0           conv2d_251[0][0]                 \n",
            "                                                                 conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 3, 1, 64)     256         add_179[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling2D) (None, 2, 1, 64)     0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_14 (Flatten)            (None, 128)          0           max_pooling2d_29[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 2)            258         flatten_14[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 577,090\n",
            "Trainable params: 575,042\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "201/201 [==============================] - 6s 21ms/step - loss: 0.9410 - accuracy: 0.5810 - val_loss: 0.6701 - val_accuracy: 0.6800\n",
            "Epoch 2/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.6821 - accuracy: 0.6596 - val_loss: 0.8201 - val_accuracy: 0.7000\n",
            "Epoch 3/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.5637 - accuracy: 0.7437 - val_loss: 0.6616 - val_accuracy: 0.7250\n",
            "Epoch 4/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.5301 - accuracy: 0.7465 - val_loss: 0.5295 - val_accuracy: 0.6900\n",
            "Epoch 5/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.4912 - accuracy: 0.7740 - val_loss: 0.4964 - val_accuracy: 0.7750\n",
            "Epoch 6/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4136 - accuracy: 0.8134 - val_loss: 0.4154 - val_accuracy: 0.8250\n",
            "Epoch 7/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.4343 - accuracy: 0.7732 - val_loss: 0.4893 - val_accuracy: 0.7300\n",
            "Epoch 8/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4321 - accuracy: 0.7969 - val_loss: 0.4408 - val_accuracy: 0.7800\n",
            "Epoch 9/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.4123 - accuracy: 0.8109 - val_loss: 0.4340 - val_accuracy: 0.7900\n",
            "Epoch 10/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.4032 - accuracy: 0.8061 - val_loss: 0.4509 - val_accuracy: 0.7550\n",
            "Epoch 11/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4051 - accuracy: 0.8197 - val_loss: 0.4268 - val_accuracy: 0.7900\n",
            "Epoch 12/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3459 - accuracy: 0.8567 - val_loss: 0.3867 - val_accuracy: 0.8000\n",
            "Epoch 13/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3305 - accuracy: 0.8659 - val_loss: 0.4024 - val_accuracy: 0.7850\n",
            "Epoch 14/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.4194 - accuracy: 0.8240 - val_loss: 0.3902 - val_accuracy: 0.8050\n",
            "Epoch 15/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3426 - accuracy: 0.8480 - val_loss: 0.4042 - val_accuracy: 0.8000\n",
            "Epoch 16/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3674 - accuracy: 0.8381 - val_loss: 0.4641 - val_accuracy: 0.7550\n",
            "Epoch 17/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3229 - accuracy: 0.8564 - val_loss: 0.4235 - val_accuracy: 0.8100\n",
            "Epoch 18/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.3740 - accuracy: 0.8535 - val_loss: 0.4003 - val_accuracy: 0.8400\n",
            "Epoch 19/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.4014 - accuracy: 0.8228 - val_loss: 0.3369 - val_accuracy: 0.8500\n",
            "Epoch 20/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2748 - accuracy: 0.9012 - val_loss: 0.3492 - val_accuracy: 0.8300\n",
            "Epoch 21/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2681 - accuracy: 0.9035 - val_loss: 0.3461 - val_accuracy: 0.8250\n",
            "Epoch 22/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.3191 - accuracy: 0.8567 - val_loss: 0.3207 - val_accuracy: 0.8600\n",
            "Epoch 23/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2887 - accuracy: 0.8782 - val_loss: 0.3489 - val_accuracy: 0.8650\n",
            "Epoch 24/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2379 - accuracy: 0.9055 - val_loss: 0.3743 - val_accuracy: 0.8500\n",
            "Epoch 25/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2700 - accuracy: 0.8965 - val_loss: 0.3362 - val_accuracy: 0.8700\n",
            "Epoch 26/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2077 - accuracy: 0.9061 - val_loss: 0.3780 - val_accuracy: 0.8150\n",
            "Epoch 27/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2455 - accuracy: 0.8965 - val_loss: 0.3586 - val_accuracy: 0.8300\n",
            "Epoch 28/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2528 - accuracy: 0.9085 - val_loss: 0.3789 - val_accuracy: 0.8250\n",
            "Epoch 29/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.2511 - accuracy: 0.9071 - val_loss: 0.3046 - val_accuracy: 0.8700\n",
            "Epoch 30/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2210 - accuracy: 0.9155 - val_loss: 0.3067 - val_accuracy: 0.8450\n",
            "Epoch 31/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2079 - accuracy: 0.9236 - val_loss: 0.3084 - val_accuracy: 0.8800\n",
            "Epoch 32/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1776 - accuracy: 0.9209 - val_loss: 0.3058 - val_accuracy: 0.8700\n",
            "Epoch 33/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2116 - accuracy: 0.9265 - val_loss: 0.3168 - val_accuracy: 0.8800\n",
            "Epoch 34/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1935 - accuracy: 0.9292 - val_loss: 0.3152 - val_accuracy: 0.8650\n",
            "Epoch 35/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1905 - accuracy: 0.9257 - val_loss: 0.3005 - val_accuracy: 0.8750\n",
            "Epoch 36/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1776 - accuracy: 0.9337 - val_loss: 0.2839 - val_accuracy: 0.8750\n",
            "Epoch 37/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1936 - accuracy: 0.9236 - val_loss: 0.3610 - val_accuracy: 0.8400\n",
            "Epoch 38/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1971 - accuracy: 0.9190 - val_loss: 0.2747 - val_accuracy: 0.8850\n",
            "Epoch 39/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1432 - accuracy: 0.9470 - val_loss: 0.2739 - val_accuracy: 0.8700\n",
            "Epoch 40/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1750 - accuracy: 0.9195 - val_loss: 0.3848 - val_accuracy: 0.8550\n",
            "Epoch 41/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2102 - accuracy: 0.9163 - val_loss: 0.2862 - val_accuracy: 0.8800\n",
            "Epoch 42/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1627 - accuracy: 0.9415 - val_loss: 0.2688 - val_accuracy: 0.8900\n",
            "Epoch 43/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1271 - accuracy: 0.9546 - val_loss: 0.3109 - val_accuracy: 0.8600\n",
            "Epoch 44/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.2090 - accuracy: 0.9311 - val_loss: 0.3765 - val_accuracy: 0.8450\n",
            "Epoch 45/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1248 - accuracy: 0.9526 - val_loss: 0.3991 - val_accuracy: 0.8650\n",
            "Epoch 46/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1802 - accuracy: 0.9245 - val_loss: 0.4322 - val_accuracy: 0.8550\n",
            "Epoch 47/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1386 - accuracy: 0.9470 - val_loss: 0.3593 - val_accuracy: 0.8700\n",
            "Epoch 48/50\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.1342 - accuracy: 0.9659 - val_loss: 0.3117 - val_accuracy: 0.8850\n",
            "Epoch 49/50\n",
            "201/201 [==============================] - 4s 18ms/step - loss: 0.1318 - accuracy: 0.9486 - val_loss: 0.3380 - val_accuracy: 0.8550\n",
            "Epoch 50/50\n",
            "201/201 [==============================] - 4s 19ms/step - loss: 0.1593 - accuracy: 0.9390 - val_loss: 0.3166 - val_accuracy: 0.8650\n",
            "Score for fold 5: loss of 0.3165506422519684; accuracy of 86.50000095367432%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.27629175782203674 - Accuracy: 90.54726362228394%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3691904544830322 - Accuracy: 84.57711338996887%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.4147241711616516 - Accuracy: 82.4999988079071%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.21556930243968964 - Accuracy: 90.49999713897705%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.3165506422519684 - Accuracy: 86.50000095367432%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 86.92487478256226 (+- 3.199225827839709)\n",
            "> Loss: 0.3184652656316757\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp0kwxfP3JMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb69670-0d6a-499c-c66a-f2cdfdc09eb6"
      },
      "source": [
        "#Creating a grouped bar plot\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "speci = (list(speci))\n",
        "sensi =  (list(sensi))\n",
        "_f1score = (list(_f1score))\n",
        "_accuracy =(list(_accuracy))\n",
        "au = (list(au))\n",
        "barWidth = 0.25\n",
        "labels = ['Specificity', 'Sensitivity', 'AUC', 'F1-score', 'Accuracy']\n",
        "cnn_means = [speci[0], sensi[0], au[0], _f1score[0], _accuracy[0]]\n",
        "cnn1_means = [speci[1], sensi[1], au[1], _f1score[1], _accuracy[1]]\n",
        "cnn2_means = [speci[2], sensi[2], au[2], _f1score[2], _accuracy[2]]\n",
        "\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "# Set position of bar on X axis\n",
        "r1 = x\n",
        "r2 = [p + barWidth for p in r1]\n",
        "r3 = [p + barWidth for p in r2]\n",
        "fig, ax = plt.subplots(figsize=(12,10))\n",
        "rects1 = ax.bar(r1, (cnn_means), width, label='XY')\n",
        "rects2 = ax.bar(r2, (cnn1_means), width, label='XZ')\n",
        "rects3 = ax.bar(r3, (cnn2_means), width, label='YZ')\n",
        "\n",
        "\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Performance Scores', fontweight='bold', fontsize = 18)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, fontweight='bold', fontsize = 16, horizontalalignment = 'left')\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{:.0%}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "autolabel(rects3)\n",
        "\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAghSQhSNhdm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}